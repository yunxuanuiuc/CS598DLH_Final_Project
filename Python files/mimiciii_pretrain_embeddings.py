# -*- coding: utf-8 -*-
"""MIMICIII_pretrain_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vu8HoxhLoRGt8YuqkH5U7S1_aUo9L1m1
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import re
import sys
import os, os.path
import shutil

from collections import Counter
import pickle
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

MIN_TOKEN_FREQ = 100
MAX_TOKENS_IN_FILE = 10000
MIN_EXAMPLES_PER_CODE = 1000
TEST_SIZE = 0.2

CPT_FILE_PATH = "drive/MyDrive/mimic-iii/CPTEVENTS.csv"
DIAGNOSIS_FILE_PATH = "drive/MyDrive/mimic-iii/DIAGNOSES_ICD.csv"
PROCEDURES_FILE_PATH = "drive/MyDrive/mimic-iii/PROCEDURES_ICD.csv"
CORPUS_FILE_PATH = "drive/MyDrive/mimic-iii/cuis"

"""# Load the MIMIC-III data and pretrain word embeddings
This first cell in this section is the same as the ICDLoader in NNModels.ipynb. In colab env, it's challenging to import classes from other py files - apologies for the duplicate code. We use this class in this notebook to pretrain a word2vec and a doc2vec model, which will be used later in NNModels.ipynb and SVM_Models.ipynb.
"""

class ICDLoader: 
  """Load ICD billing codes labels for each patient"""

  def __init__(self, corpus_file_path, cpt_file_path, diagnosis_file_path, procedures_file_path, min_examples_per_code, min_token_freq, max_tokens_in_file):
    self.corpus_path = corpus_file_path
    self.cpt_path = cpt_file_path
    self.diagnosis_path = diagnosis_file_path
    self.procedures_path = procedures_file_path
   
    self.max_tokens_in_file = max_tokens_in_file
    self.min_examples_per_code = min_examples_per_code
    self.min_token_freq = min_token_freq
    self.patient2label_dict = None #mapping from patient -> ICD9 label codes
    self.label2idx_dict = None #mapping from ICD9 label code -> embedding idx

    self.token2int = {}

  def df_make_code(self, df, icd_type, code_len, code_col):
    #making special short string for codes
    df['short_code'] = icd_type + "_" + df[code_col].astype(str).str[:code_len]
    return df
  
  def get_label2freq_df(self, df, min_examples_per_code):
    #get a filtered label codes -> frequency mapping table 
    label2freq_df = df[["SUBJECT_ID", "short_code"]].groupby("short_code").nunique()
    label2freq_df = label2freq_df[label2freq_df["SUBJECT_ID"]>min_examples_per_code]
    label2freq_df.rename({"SUBJECT_ID": "freq"}, axis=1, inplace=True)
    return label2freq_df.reset_index()

  def get_patient2label_df(self, df, label2freq_df):
    #get df of patient mapping to all their filtered ICD9 code labels (filtered by freq)
    df_filtered = df[df["short_code"].isin(label2freq_df["short_code"])] 
    patient2label_df = df_filtered[["SUBJECT_ID", "short_code"]]\
                          .groupby("SUBJECT_ID")\
                          .agg({'short_code':lambda sf: set(sf)})
    patient2label_df.rename({"short_code":"short_codes"}, axis=1, inplace=True)          
    return patient2label_df.reset_index()

  def create_patient_label_vec(self, subj_id, patient2label_dict, label2idx_dict):
    #make patient label vector
    code_vec = [0]*len(label2idx_dict)
    codes = patient2label_dict[subj_id]
    for code in codes:
      code_vec[label2idx_dict[code]] = 1
    return code_vec

  def make_cui_token2int_mapping(self):
    #count tokens
    i=0
    token_count_dict = {}
    for file in os.listdir(self.corpus_path):
      text = open(os.path.join(self.corpus_path,file)).read()
      tokens = [token for token in text.split()] #assume all cui in file splitted by space
      if len(tokens) > self.max_tokens_in_file:
        continue
      else:
        for token in tokens:
          if token in token_count_dict:
            token_count_dict[token] += 1
          else:
            token_count_dict[token] = 1
    
    #make token2int mapping
    oov_idx = 0
    idx = 1
    self.token2int['oov_word'] = 0
    for token, count in token_count_dict.items():
      if count > self.min_token_freq:
        self.token2int[token] = idx
        idx += 1
  
  def create_cui_input_sequence(self, tokens):
    #create cui_input_sequence from cui tokens
    input = []
    tokens_set = set(tokens)

    for token in tokens_set:
      if token in self.token2int:
        input.append(self.token2int[token])
      else:
        input.append(self.token2int['oov_word'])

    return input

  def run(self):
    #run everything

    #codes init
    cpt = pd.read_csv(self.cpt_path)
    diagnosis = pd.read_csv(self.diagnosis_path)
    procedures = pd.read_csv(self.procedures_path)

    #codes init
    cpt = self.df_make_code(cpt, 'cpt', 5, 'CPT_NUMBER')
    diagnosis = self.df_make_code(diagnosis, 'diag', 3, 'ICD9_CODE')
    procedures = self.df_make_code(procedures, 'proc', 2, 'ICD9_CODE')

    #codes init
    all_codes = cpt[["SUBJECT_ID", "short_code"]]\
              .append(diagnosis[["SUBJECT_ID", "short_code"]], ignore_index=True)\
              .append(procedures[["SUBJECT_ID", "short_code"]], ignore_index=True)

    #codes init
    label2freq_df = self.get_label2freq_df(all_codes, self.min_examples_per_code)
    patient2label_df = self.get_patient2label_df(all_codes, label2freq_df)
    
    #codes init
    self.label2idx_dict = dict(label2freq_df.reset_index()[["short_code", "index"]].values)
    self.patient2label_dict = dict(patient2label_df.values)

    #cui init
    self.make_cui_token2int_mapping()

    codes = []
    cui_inputs = []

    #processing
    for file in os.listdir(self.corpus_path): #list files to run
      text = open(os.path.join(self.corpus_path, file)).read()
      tokens = [token for token in text.split()]
      if len(tokens) > self.max_tokens_in_file: #cui filter
        continue

      subj_id = int(re.findall('\d+(?=.txt)', file)[0])
      if subj_id not in self.patient2label_dict: #icd9 filter
        continue

      #icd9 process  
      code_vec = self.create_patient_label_vec(subj_id, self.patient2label_dict, 
                                               self.label2idx_dict)
      if sum(code_vec) == 0:
        continue
      codes.append(code_vec)

      #cui process
      cui_input = self.create_cui_input_sequence(tokens)
      cui_inputs.append(cui_input)


    return cui_inputs, codes

loader = ICDLoader(CORPUS_FILE_PATH, CPT_FILE_PATH, DIAGNOSIS_FILE_PATH, PROCEDURES_FILE_PATH, MIN_EXAMPLES_PER_CODE, MIN_TOKEN_FREQ, MAX_TOKENS_IN_FILE)
cui_inputs, codes = loader.run()

# saving dictionary to avoid processing 46k+ files everytime
save_dict_file = open("drive/MyDrive/mimic-iii/token2int_dict.pkl", "wb")
pickle.dump(loader.token2int, save_dict_file)
save_dict_file.close()

with open("drive/MyDrive/mimic-iii/token2int_dict.pkl", "rb") as fp:
  token2int_dict = pickle.load(fp)

# with open("drive/MyDrive/mimic-iii/patient2label_dict.pkl", "wb") as fp:   #Pickling
#   pickle.dump(loader.patient2label_dict, fp)

# with open("drive/MyDrive/mimic-iii/label2idx_dict.pkl", "wb") as fp:   #Pickling
#   pickle.dump(loader.label2idx_dict, fp)

# with open("drive/MyDrive/mimic-iii/cui_inputs.pkl", "wb") as fp:   #Pickling
#   pickle.dump(cui_inputs, fp)

# with open("drive/MyDrive/mimic-iii/icd9codes.pkl", "wb") as fp:   #Pickling
#   pickle.dump(codes, fp)

with open("drive/MyDrive/mimic-iii/cui_inputs.pkl", "rb") as fp:   #Pickling
  cui_inputs = pickle.load(fp)

with open("drive/MyDrive/mimic-iii/icd9codes.pkl", "rb") as fp:   #Pickling
  codes = pickle.load(fp)

len(cui_inputs), len(codes)

"""## Word2Vec Model"""

cui_inputs_str = [[str(x) for x in cui_input] for cui_input in cui_inputs]

len(cui_inputs_str)

model = Word2Vec(sentences=cui_inputs_str, window=5, size=300, workers=4, min_count=1)

model.wv.vectors

model.save("drive/MyDrive/mimic-iii/word2vec_2.model")
# model = Word2Vec.load("drive/MyDrive/mimic-iii/word2vec.model")



"""## Doc2Vec model"""

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(cui_inputs_str)]
model = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=4)

model.save("drive/MyDrive/MIMIC/mimic-iii/doc2vec.model")

