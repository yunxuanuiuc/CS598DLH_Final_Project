{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNModels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7d3ARDXLrFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe419729-1ba8-4d58-89c9-6c73ad76cddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the Neural Network model proposed\n",
        "In this notebook, we construct the NN model as discussed in section 2.1 of the original paper. We build the model, and fit the model with MIMIC-III data to try to predict ICD9 codes.\n",
        "The model will be used later to generate patient representations (from the hidden layer) for disease prediction task."
      ],
      "metadata": {
        "id": "VCA7DwVERlwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "M0O3_WWD1pxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Na-LsSzxqiff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction configs:\n",
        "MIN_TOKEN_FREQ = 100\n",
        "MAX_TOKENS_IN_FILE = 10000\n",
        "MIN_EXAMPLES_PER_CODE = 1000\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "#yunxuan's\n",
        "CPT_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/CPTEVENTS.csv\"\n",
        "DIAGNOSIS_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "PROCEDURES_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "CORPUS_FILE_PATH = \"drive/MyDrive/MIMIC/output_first_half_selected_cuis/\" #FILL IN PATH\n",
        "\n",
        "CPT_FILE_PATH = \"drive/MyDrive/mimic-iii/CPTEVENTS.csv\"\n",
        "DIAGNOSIS_FILE_PATH = \"drive/MyDrive/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "PROCEDURES_FILE_PATH = \"drive/MyDrive/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "CORPUS_FILE_PATH = \"drive/MyDrive/mimic-iii/cuis/\" #FILL IN PATH"
      ],
      "metadata": {
        "id": "qYtRDkM41rcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model configs\n",
        "criterion = nn.BCELoss()\n",
        "n_epochs = 75\n",
        "batch_size = 50\n",
        "\n",
        "#word2vec config\n",
        "USE_PRETRAINED_W2V = False\n",
        "RUN_LOADER = False"
      ],
      "metadata": {
        "id": "OVDwacnI9cb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FlyxVPVRWRvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the MIMIC-III data\n",
        "Note that clinical notes were preprocessed via apache ctakes (check ctakes script and , and Concept Unique Identifiers (CUIs) were already extracted. The ICDLoader takes each patient's file of CUIs and loads the data."
      ],
      "metadata": {
        "id": "XNtOww5gWSfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ICDLoader: \n",
        "  \"\"\"Load ICD billing codes labels for each patient\"\"\"\n",
        "\n",
        "  def __init__(self, corpus_file_path, cpt_file_path, diagnosis_file_path, procedures_file_path, min_examples_per_code, min_token_freq, max_tokens_in_file):\n",
        "    self.corpus_path = corpus_file_path\n",
        "    self.cpt_path = cpt_file_path\n",
        "    self.diagnosis_path = diagnosis_file_path\n",
        "    self.procedures_path = procedures_file_path\n",
        "   \n",
        "    self.max_tokens_in_file = max_tokens_in_file\n",
        "    self.min_examples_per_code = min_examples_per_code\n",
        "    self.min_token_freq = min_token_freq\n",
        "    self.patient2label_dict = None #mapping from patient -> ICD9 label codes\n",
        "    self.label2idx_dict = None #mapping from ICD9 label code -> embedding idx\n",
        "\n",
        "    self.token2int = {}\n",
        "\n",
        "  def df_make_code(self, df, icd_type, code_len, code_col):\n",
        "    #making special short string for codes\n",
        "    df['short_code'] = icd_type + \"_\" + df[code_col].astype(str).str[:code_len]\n",
        "    return df\n",
        "  \n",
        "  def get_label2freq_df(self, df, min_examples_per_code):\n",
        "    #get a filtered label codes -> frequency mapping table \n",
        "    label2freq_df = df[[\"SUBJECT_ID\", \"short_code\"]].groupby(\"short_code\").nunique()\n",
        "    label2freq_df = label2freq_df[label2freq_df[\"SUBJECT_ID\"]>min_examples_per_code]\n",
        "    label2freq_df.rename({\"SUBJECT_ID\": \"freq\"}, axis=1, inplace=True)\n",
        "    return label2freq_df.reset_index()\n",
        "\n",
        "  def get_patient2label_df(self, df, label2freq_df):\n",
        "    #get df of patient mapping to all their filtered ICD9 code labels (filtered by freq)\n",
        "    df_filtered = df[df[\"short_code\"].isin(label2freq_df[\"short_code\"])] \n",
        "    patient2label_df = df_filtered[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "                          .groupby(\"SUBJECT_ID\")\\\n",
        "                          .agg({'short_code':lambda sf: set(sf)})\n",
        "    patient2label_df.rename({\"short_code\":\"short_codes\"}, axis=1, inplace=True)          \n",
        "    return patient2label_df.reset_index()\n",
        "\n",
        "  def create_patient_label_vec(self, subj_id, patient2label_dict, label2idx_dict):\n",
        "    #make patient label vector\n",
        "    code_vec = [0]*len(label2idx_dict)\n",
        "    codes = patient2label_dict[subj_id]\n",
        "    for code in codes:\n",
        "      code_vec[label2idx_dict[code]] = 1\n",
        "    return code_vec\n",
        "\n",
        "  def make_cui_token2int_mapping(self):\n",
        "    #count tokens\n",
        "    token_count_dict = {}\n",
        "    for file in os.listdir(self.corpus_path):\n",
        "      text = open(os.path.join(self.corpus_path,file)).read()\n",
        "      tokens = [token for token in text.split()] #assume all cui in file splitted by space\n",
        "      if len(tokens) > self.max_tokens_in_file:\n",
        "        continue\n",
        "      else:\n",
        "        for token in tokens:\n",
        "          if token in token_count_dict:\n",
        "            token_count_dict[token] += 1\n",
        "          else:\n",
        "            token_count_dict[token] = 1\n",
        "    \n",
        "    #make token2int mapping\n",
        "    oov_idx = 0\n",
        "    idx = 1\n",
        "    self.token2int['oov_word'] = 0\n",
        "    for token, count in token_count_dict.items():\n",
        "      if count > self.min_token_freq:\n",
        "        self.token2int[token] = idx\n",
        "        idx += 1\n",
        "  \n",
        "  def create_cui_input_sequence(self, tokens):\n",
        "    #create cui_input_sequence from cui tokens\n",
        "    input = []\n",
        "    tokens_set = set(tokens)\n",
        "\n",
        "    for token in tokens_set:\n",
        "      if token in self.token2int:\n",
        "        input.append(self.token2int[token])\n",
        "      else:\n",
        "        input.append(self.token2int['oov_word'])\n",
        "\n",
        "    return input\n",
        "\n",
        "  def run(self):\n",
        "    #run everything\n",
        "\n",
        "    #codes init\n",
        "    cpt = pd.read_csv(self.cpt_path)\n",
        "    diagnosis = pd.read_csv(self.diagnosis_path)\n",
        "    procedures = pd.read_csv(self.procedures_path)\n",
        "\n",
        "    #codes init\n",
        "    cpt = self.df_make_code(cpt, 'cpt', 5, 'CPT_NUMBER')\n",
        "    diagnosis = self.df_make_code(diagnosis, 'diag', 3, 'ICD9_CODE')\n",
        "    procedures = self.df_make_code(procedures, 'proc', 2, 'ICD9_CODE')\n",
        "\n",
        "    #codes init\n",
        "    all_codes = cpt[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "              .append(diagnosis[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\\\n",
        "              .append(procedures[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\n",
        "\n",
        "    #codes init\n",
        "    label2freq_df = self.get_label2freq_df(all_codes, self.min_examples_per_code)\n",
        "    patient2label_df = self.get_patient2label_df(all_codes, label2freq_df)\n",
        "    \n",
        "    #codes init\n",
        "    self.label2idx_dict = dict(label2freq_df.reset_index()[[\"short_code\", \"index\"]].values)\n",
        "    self.patient2label_dict = dict(patient2label_df.values)\n",
        "\n",
        "    #cui init\n",
        "    # self.make_cui_token2int_mapping()\n",
        "    with open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"rb\") as fp:\n",
        "      self.token2int = pickle.load(fp)\n",
        "\n",
        "    codes = []\n",
        "    cui_inputs = []\n",
        "\n",
        "    #processing\n",
        "    for file in os.listdir(self.corpus_path): #list files to run\n",
        "      text = open(os.path.join(self.corpus_path, file)).read()\n",
        "      tokens = [token for token in text.split()]\n",
        "      if len(tokens) > self.max_tokens_in_file: #cui filter\n",
        "        continue\n",
        "\n",
        "      subj_id = int(re.findall('\\d+(?=.txt)', file)[0])\n",
        "      if subj_id not in self.patient2label_dict: #icd9 filter\n",
        "        continue\n",
        "\n",
        "      #icd9 process  \n",
        "      code_vec = self.create_patient_label_vec(subj_id, self.patient2label_dict, \n",
        "                                               self.label2idx_dict)\n",
        "      if sum(code_vec) == 0:\n",
        "        continue\n",
        "      codes.append(code_vec)\n",
        "\n",
        "      #cui process\n",
        "      cui_input = self.create_cui_input_sequence(tokens)\n",
        "      cui_inputs.append(cui_input)\n",
        "\n",
        "    return cui_inputs, codes"
      ],
      "metadata": {
        "id": "vNZSYMog1y7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after the first run, we dumped all critical attributes of ICDLoader so we only need to load them later, avoiding processing the giant MIMIC-III dataset again and again.\n",
        "if RUN_LOADER:\n",
        "  loader = ICDLoader(CORPUS_FILE_PATH, CPT_FILE_PATH, DIAGNOSIS_FILE_PATH, PROCEDURES_FILE_PATH, MIN_EXAMPLES_PER_CODE, MIN_TOKEN_FREQ, MAX_TOKENS_IN_FILE)\n",
        "  cui_inputs, codes = loader.run()"
      ],
      "metadata": {
        "id": "QChkvpCd10_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"rb\") as fp:\n",
        "  token2int = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/cui_inputs.pkl\", \"rb\") as fp:   #Pickling\n",
        "  cui_inputs = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/icd9codes.pkl\", \"rb\") as fp:   #Pickling\n",
        "  codes = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/patient2label_dict.pkl\", \"rb\") as fp:   #Pickling\n",
        "  patient2label_dict = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/label2idx_dict.pkl\", \"rb\") as fp:   #Pickling\n",
        "  label2idx_dict = pickle.load(fp)"
      ],
      "metadata": {
        "id": "vh3qv31PDiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RHr-af9XWL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the model"
      ],
      "metadata": {
        "id": "euR2U5ycWNZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = max([len(patient) for patient in cui_inputs])\n",
        "emb_dim = len(token2int)\n",
        "n_class = len(label2idx_dict)"
      ],
      "metadata": {
        "id": "5asmd-8daX_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-JcVtdG6cevL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cui_inputs,codes, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sFdSksIsWSrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x # shape n_sample x padded length\n",
        "        self.y = y # shape n_sample x n classes [[0,1,0,0], [1,1,1,0]]\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        return(self.x[index], self.y[index])"
      ],
      "metadata": {
        "id": "ZN4e24eH7ESQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = customDataset(X_train, y_train)\n",
        "test_dataset = customDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "eYi0vuIRXt4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the paper discussed training 2 versions of the model: (1) with randomly initialized CUI embeddings, (2) with word2vec-pretrained CUI embeddings\n",
        "if USE_PRETRAINED_W2V:\n",
        "  w2v_model = Word2Vec.load(\"drive/MyDrive/mimic-iii/word2vec.model\")"
      ],
      "metadata": {
        "id": "GYOUvu5wcNKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "    n = len(sequences)\n",
        "    x = torch.zeros((n, maxlen), dtype=torch.long).to(device)\n",
        "    \n",
        "    for patient, cuis in enumerate(sequences):\n",
        "      len_cuis = len(cuis)\n",
        "      x[patient][:len_cuis] = torch.tensor(cuis).to(device)\n",
        "      \n",
        "    return x, y\n",
        "\n",
        "def collate_fn_w2v(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "    n = len(sequences)\n",
        "    x = torch.zeros((n, maxlen), dtype=torch.long).to(device)\n",
        "    \n",
        "    for patient, cuis in enumerate(sequences):\n",
        "      len_cuis = len(cuis)\n",
        "      cui_w2v_idx = [w2v_model.wv.vocab[str(token)].index for token in cuis]\n",
        "      x[patient][:len_cuis] = torch.tensor(cui_w2v_idx).to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "jEE-XLdKWm0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_W2V:\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "else:\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_w2v)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_w2v)"
      ],
      "metadata": {
        "id": "pLvc4ost7lCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_representation(nn.Module):\n",
        "  def __init__(self, in_dim, n_diseases, w2v_model=None):\n",
        "    super(NN_representation, self).__init__()\n",
        "    if w2v_model:\n",
        "      weights = torch.FloatTensor(w2v_model.wv.vectors)\n",
        "      self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300).from_pretrained(weights, freeze=False)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300)\n",
        "    self.avg = nn.AdaptiveMaxPool1d(1)\n",
        "    \n",
        "    self.hidden = nn.Linear(300, 1000)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.final = nn.Linear(1000, n_diseases)\n",
        "    self.act2 = nn.Sigmoid()\n",
        "\n",
        "  def get_hidden(self, x):\n",
        "    temp = self.emb(x)\n",
        "    #print(f\"after emb, {temp.shape}\")\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    #print(f\"after permute, {temp.shape}\")\n",
        "    temp = self.avg(temp)\n",
        "    #print(f\"after avg, {temp.shape}\")\n",
        "    temp = temp.squeeze(-1)\n",
        "    #print(f\"after squeeze, {temp.shape}\")\n",
        "    h = self.hidden(temp)\n",
        "    return h\n",
        "\n",
        "  def forward(self, x):\n",
        "    temp = self.emb(x)\n",
        "    #print(f\"after emb, {temp.shape}\")\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    #print(f\"after permute, {temp.shape}\")\n",
        "    temp = self.avg(temp)\n",
        "    #print(f\"after avg, {temp.shape}\")\n",
        "    temp = temp.squeeze(-1)\n",
        "    #print(f\"after squeeze, {temp.shape}\")\n",
        "    temp = self.hidden(temp)\n",
        "    #print(f\"after hidden, {temp.shape}\")\n",
        "    temp = self.act1(temp)\n",
        "    #print(f\"after relu, {temp.shape}\")\n",
        "    temp = self.final(temp)\n",
        "    #print(f\"after linear hidden, {temp.shape}\")\n",
        "    res = self.act2(temp)\n",
        "    return res"
      ],
      "metadata": {
        "id": "RbIUQDSd0TGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_W2V:\n",
        "  nnmodel =NN_representation(emb_dim, n_class).to(device)\n",
        "else:\n",
        "  nnmodel =NN_representation(emb_dim, n_class, w2v_model).to(device)\n",
        "optimizer = torch.optim.RMSprop(nnmodel.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tO36rB2kaAyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pzdakpOuWGqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation"
      ],
      "metadata": {
        "id": "CsJbyRccWHR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, n_epochs):\n",
        "  model.train()\n",
        "  for epoch in range(n_epochs):\n",
        "    current_loss = 0\n",
        "    for current_x, current_y in loader:\n",
        "      pred = model(current_x)\n",
        "      loss = criterion(pred, current_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    train_loss = current_loss/len(loader)\n",
        "    print(f\"after epoch {epoch}, the training loss is {train_loss}\")\n"
      ],
      "metadata": {
        "id": "Tf7_D2aaw2rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(nnmodel, train_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAqZDtAcgdAN",
        "outputId": "7b18646b-3538-4a98-8c3d-f9f19fe8427c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after epoch 0, the training loss is 0.23171569080324145\n",
            "after epoch 1, the training loss is 0.1964738229597295\n",
            "after epoch 2, the training loss is 0.18672735414079003\n",
            "after epoch 3, the training loss is 0.18082733855054184\n",
            "after epoch 4, the training loss is 0.17627065711551243\n",
            "after epoch 5, the training loss is 0.17274117870284272\n",
            "after epoch 6, the training loss is 0.16948758642952721\n",
            "after epoch 7, the training loss is 0.1669167976144019\n",
            "after epoch 8, the training loss is 0.16434563336161165\n",
            "after epoch 9, the training loss is 0.16214651942208363\n",
            "after epoch 10, the training loss is 0.16044329180642292\n",
            "after epoch 11, the training loss is 0.15877970615709508\n",
            "after epoch 12, the training loss is 0.1571497179388821\n",
            "after epoch 13, the training loss is 0.15592614543420058\n",
            "after epoch 14, the training loss is 0.1547094414474907\n",
            "after epoch 15, the training loss is 0.15357521326244772\n",
            "after epoch 16, the training loss is 0.15254789065043847\n",
            "after epoch 17, the training loss is 0.15178963954786998\n",
            "after epoch 18, the training loss is 0.1508818919079619\n",
            "after epoch 19, the training loss is 0.14994316300875074\n",
            "after epoch 20, the training loss is 0.14910867710684514\n",
            "after epoch 21, the training loss is 0.14829996388193006\n",
            "after epoch 22, the training loss is 0.14756332602989566\n",
            "after epoch 23, the training loss is 0.14672687194413608\n",
            "after epoch 24, the training loss is 0.14634835498558507\n",
            "after epoch 25, the training loss is 0.14569089942060792\n",
            "after epoch 26, the training loss is 0.14516129041532497\n",
            "after epoch 27, the training loss is 0.14476610380071062\n",
            "after epoch 28, the training loss is 0.1443660963024642\n",
            "after epoch 29, the training loss is 0.14397057434460064\n",
            "after epoch 30, the training loss is 0.1434707015909411\n",
            "after epoch 31, the training loss is 0.143160181405308\n",
            "after epoch 32, the training loss is 0.1427936734626064\n",
            "after epoch 33, the training loss is 0.14260401448106264\n",
            "after epoch 34, the training loss is 0.1422842045625051\n",
            "after epoch 35, the training loss is 0.14214628928147996\n",
            "after epoch 36, the training loss is 0.1416565982831849\n",
            "after epoch 37, the training loss is 0.14146345624470855\n",
            "after epoch 38, the training loss is 0.1413510077305742\n",
            "after epoch 39, the training loss is 0.14101036488114893\n",
            "after epoch 40, the training loss is 0.14064039160181452\n",
            "after epoch 41, the training loss is 0.14058168330887058\n",
            "after epoch 42, the training loss is 0.14026997299926416\n",
            "after epoch 43, the training loss is 0.1399703731944969\n",
            "after epoch 44, the training loss is 0.1396973755199451\n",
            "after epoch 45, the training loss is 0.13960679084644304\n",
            "after epoch 46, the training loss is 0.13928405743640465\n",
            "after epoch 47, the training loss is 0.1391369087604789\n",
            "after epoch 48, the training loss is 0.1389549828551195\n",
            "after epoch 49, the training loss is 0.1386859405215259\n",
            "after epoch 50, the training loss is 0.13854170640980876\n",
            "after epoch 51, the training loss is 0.1383598590823444\n",
            "after epoch 52, the training loss is 0.13826892043914166\n",
            "after epoch 53, the training loss is 0.13809892086608633\n",
            "after epoch 54, the training loss is 0.137891413515305\n",
            "after epoch 55, the training loss is 0.13775145343026599\n",
            "after epoch 56, the training loss is 0.13758459764423672\n",
            "after epoch 57, the training loss is 0.13741969326147446\n",
            "after epoch 58, the training loss is 0.1374750052307461\n",
            "after epoch 59, the training loss is 0.13725724651872576\n",
            "after epoch 60, the training loss is 0.13709173499195426\n",
            "after epoch 61, the training loss is 0.1369395542990517\n",
            "after epoch 62, the training loss is 0.13684730935427877\n",
            "after epoch 63, the training loss is 0.13664175816126414\n",
            "after epoch 64, the training loss is 0.13656378851772788\n",
            "after epoch 65, the training loss is 0.13635306017803359\n",
            "after epoch 66, the training loss is 0.1362538421610454\n",
            "after epoch 67, the training loss is 0.13623906046644346\n",
            "after epoch 68, the training loss is 0.1360052915194908\n",
            "after epoch 69, the training loss is 0.13595588803962544\n",
            "after epoch 70, the training loss is 0.13595035262741484\n",
            "after epoch 71, the training loss is 0.1357260943577812\n",
            "after epoch 72, the training loss is 0.1356822774932907\n",
            "after epoch 73, the training loss is 0.13548288642017692\n",
            "after epoch 74, the training loss is 0.13543696844720984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, loader):\n",
        "  model.eval()\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for current_x, current_y in loader:\n",
        "      preds = model(current_x).cpu().detach().numpy()\n",
        "      preds_labels = preds>0.5\n",
        "      y_pred.append(preds_labels)\n",
        "      y_true.append(current_y.cpu().numpy())\n",
        "  y_pred = np.vstack(y_pred)  \n",
        "  y_true = np.vstack(y_true)        \n",
        "\n",
        "  p,r,f,_ = precision_recall_fscore_support(y_pred, y_true, average='macro')\n",
        "  acc = accuracy_score(y_pred, y_true)\n",
        "  return p,r,f,acc"
      ],
      "metadata": {
        "id": "mEsfpNvy26K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy = test(nnmodel, test_loader)"
      ],
      "metadata": {
        "id": "LVkdbKlqkbpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda297ef-5df7-4202-821e-7ea054ca791f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irc9lEZUlG5H",
        "outputId": "fdbefff0-302a-4091-8df3-b0923678515f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3144979075718745,\n",
              " 0.567808164916165,\n",
              " 0.36195371706080337,\n",
              " 0.04204709274387314)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_train, recall_train, f1_train, accuracy_train = test(nnmodel, train_loader)"
      ],
      "metadata": {
        "id": "bwnoyfB18DRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac85bf8-6832-4c41-8199-d6eae75b0313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_train, recall_train, f1_train, accuracy_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MazX9aY5luaN",
        "outputId": "ed7fcaf6-7c55-45d6-964e-bea2795cad53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16600159921033597,\n",
              " 0.36324889164086754,\n",
              " 0.18232091677587953,\n",
              " 0.044003243925149434)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eEOgJlqlV967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model for tasks in SVM_Models notebook."
      ],
      "metadata": {
        "id": "ENlRLBn9V_tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(nnmodel.state_dict(), \"drive/MyDrive/mimic-iii/nnmodel_pretrained_w2v_state_dict.pt\")\n",
        "nnmodel_new =NN_representation(emb_dim, n_class).to(device)\n",
        "nnmodel_new.load_state_dict(torch.load(\"drive/MyDrive/mimic-iii/nnmodel_pretrained_w2v_state_dict.pt\"))"
      ],
      "metadata": {
        "id": "py99aGDTW4mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G2BxtO-OcBgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}