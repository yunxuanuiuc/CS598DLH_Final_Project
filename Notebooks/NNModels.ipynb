{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNModels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7d3ARDXLrFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8087f66-d5a9-4c8f-aae0-bbddc6106914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the Neural Network model proposed\n",
        "In this notebook, we construct the NN model as discussed in section 2.1 of the original paper. We build the model, and fit the model with MIMIC-III data to try to predict ICD9 codes.\n",
        "The model will be used later to generate patient representations (from the hidden layer) for disease prediction task."
      ],
      "metadata": {
        "id": "VCA7DwVERlwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "M0O3_WWD1pxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Na-LsSzxqiff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction configs:\n",
        "MIN_TOKEN_FREQ = 100\n",
        "MAX_TOKENS_IN_FILE = 10000\n",
        "MIN_EXAMPLES_PER_CODE = 1000\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "#yunxuan's\n",
        "CPT_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/CPTEVENTS.csv\"\n",
        "DIAGNOSIS_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "PROCEDURES_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "CORPUS_FILE_PATH = \"drive/MyDrive/MIMIC/output_first_half_selected_cuis/\" #FILL IN PATH\n",
        "\n",
        "CPT_FILE_PATH = \"drive/MyDrive/mimic-iii/CPTEVENTS.csv\"\n",
        "DIAGNOSIS_FILE_PATH = \"drive/MyDrive/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "PROCEDURES_FILE_PATH = \"drive/MyDrive/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "CORPUS_FILE_PATH = \"drive/MyDrive/mimic-iii/cuis/\" #FILL IN PATH"
      ],
      "metadata": {
        "id": "qYtRDkM41rcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model configs\n",
        "criterion = nn.BCELoss()\n",
        "n_epochs = 75\n",
        "batch_size = 50\n",
        "\n",
        "#word2vec config\n",
        "USE_PRETRAINED_W2V = False\n",
        "RUN_LOADER = False"
      ],
      "metadata": {
        "id": "OVDwacnI9cb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FlyxVPVRWRvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the MIMIC-III data\n",
        "Note that clinical notes were preprocessed via apache ctakes (check ctakes script and , and Concept Unique Identifiers (CUIs) were already extracted. The ICDLoader takes each patient's file of CUIs and loads the data."
      ],
      "metadata": {
        "id": "XNtOww5gWSfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ICDLoader: \n",
        "  \"\"\"Load ICD billing codes labels for each patient\"\"\"\n",
        "\n",
        "  def __init__(self, corpus_file_path, cpt_file_path, diagnosis_file_path, procedures_file_path, min_examples_per_code, min_token_freq, max_tokens_in_file):\n",
        "    self.corpus_path = corpus_file_path\n",
        "    self.cpt_path = cpt_file_path\n",
        "    self.diagnosis_path = diagnosis_file_path\n",
        "    self.procedures_path = procedures_file_path\n",
        "   \n",
        "    self.max_tokens_in_file = max_tokens_in_file\n",
        "    self.min_examples_per_code = min_examples_per_code\n",
        "    self.min_token_freq = min_token_freq\n",
        "    self.patient2label_dict = None #mapping from patient -> ICD9 label codes\n",
        "    self.label2idx_dict = None #mapping from ICD9 label code -> embedding idx\n",
        "\n",
        "    self.token2int = {}\n",
        "\n",
        "  def df_make_code(self, df, icd_type, code_len, code_col):\n",
        "    #making special short string for codes\n",
        "    df['short_code'] = icd_type + \"_\" + df[code_col].astype(str).str[:code_len]\n",
        "    return df\n",
        "  \n",
        "  def get_label2freq_df(self, df, min_examples_per_code):\n",
        "    #get a filtered label codes -> frequency mapping table \n",
        "    label2freq_df = df[[\"SUBJECT_ID\", \"short_code\"]].groupby(\"short_code\").nunique()\n",
        "    label2freq_df = label2freq_df[label2freq_df[\"SUBJECT_ID\"]>min_examples_per_code]\n",
        "    label2freq_df.rename({\"SUBJECT_ID\": \"freq\"}, axis=1, inplace=True)\n",
        "    return label2freq_df.reset_index()\n",
        "\n",
        "  def get_patient2label_df(self, df, label2freq_df):\n",
        "    #get df of patient mapping to all their filtered ICD9 code labels (filtered by freq)\n",
        "    df_filtered = df[df[\"short_code\"].isin(label2freq_df[\"short_code\"])] \n",
        "    patient2label_df = df_filtered[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "                          .groupby(\"SUBJECT_ID\")\\\n",
        "                          .agg({'short_code':lambda sf: set(sf)})\n",
        "    patient2label_df.rename({\"short_code\":\"short_codes\"}, axis=1, inplace=True)          \n",
        "    return patient2label_df.reset_index()\n",
        "\n",
        "  def create_patient_label_vec(self, subj_id, patient2label_dict, label2idx_dict):\n",
        "    #make patient label vector\n",
        "    code_vec = [0]*len(label2idx_dict)\n",
        "    codes = patient2label_dict[subj_id]\n",
        "    for code in codes:\n",
        "      code_vec[label2idx_dict[code]] = 1\n",
        "    return code_vec\n",
        "\n",
        "  def make_cui_token2int_mapping(self):\n",
        "    #count tokens\n",
        "    token_count_dict = {}\n",
        "    for file in os.listdir(self.corpus_path):\n",
        "      text = open(os.path.join(self.corpus_path,file)).read()\n",
        "      tokens = [token for token in text.split()] #assume all cui in file splitted by space\n",
        "      if len(tokens) > self.max_tokens_in_file:\n",
        "        continue\n",
        "      else:\n",
        "        for token in tokens:\n",
        "          if token in token_count_dict:\n",
        "            token_count_dict[token] += 1\n",
        "          else:\n",
        "            token_count_dict[token] = 1\n",
        "    \n",
        "    #make token2int mapping\n",
        "    oov_idx = 0\n",
        "    idx = 1\n",
        "    self.token2int['oov_word'] = 0\n",
        "    for token, count in token_count_dict.items():\n",
        "      if count > self.min_token_freq:\n",
        "        self.token2int[token] = idx\n",
        "        idx += 1\n",
        "  \n",
        "  def create_cui_input_sequence(self, tokens):\n",
        "    #create cui_input_sequence from cui tokens\n",
        "    input = []\n",
        "    tokens_set = set(tokens)\n",
        "\n",
        "    for token in tokens_set:\n",
        "      if token in self.token2int:\n",
        "        input.append(self.token2int[token])\n",
        "      else:\n",
        "        input.append(self.token2int['oov_word'])\n",
        "\n",
        "    return input\n",
        "\n",
        "  def run(self):\n",
        "    #run everything\n",
        "\n",
        "    #codes init\n",
        "    cpt = pd.read_csv(self.cpt_path)\n",
        "    diagnosis = pd.read_csv(self.diagnosis_path)\n",
        "    procedures = pd.read_csv(self.procedures_path)\n",
        "\n",
        "    #codes init\n",
        "    cpt = self.df_make_code(cpt, 'cpt', 5, 'CPT_NUMBER')\n",
        "    diagnosis = self.df_make_code(diagnosis, 'diag', 3, 'ICD9_CODE')\n",
        "    procedures = self.df_make_code(procedures, 'proc', 2, 'ICD9_CODE')\n",
        "\n",
        "    #codes init\n",
        "    all_codes = cpt[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "              .append(diagnosis[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\\\n",
        "              .append(procedures[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\n",
        "\n",
        "    #codes init\n",
        "    label2freq_df = self.get_label2freq_df(all_codes, self.min_examples_per_code)\n",
        "    patient2label_df = self.get_patient2label_df(all_codes, label2freq_df)\n",
        "    \n",
        "    #codes init\n",
        "    self.label2idx_dict = dict(label2freq_df.reset_index()[[\"short_code\", \"index\"]].values)\n",
        "    self.patient2label_dict = dict(patient2label_df.values)\n",
        "\n",
        "    #cui init\n",
        "    # self.make_cui_token2int_mapping()\n",
        "    with open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"rb\") as fp:\n",
        "      self.token2int = pickle.load(fp)\n",
        "\n",
        "    codes = []\n",
        "    cui_inputs = []\n",
        "\n",
        "    #processing\n",
        "    for file in os.listdir(self.corpus_path): #list files to run\n",
        "      text = open(os.path.join(self.corpus_path, file)).read()\n",
        "      tokens = [token for token in text.split()]\n",
        "      if len(tokens) > self.max_tokens_in_file: #cui filter\n",
        "        continue\n",
        "\n",
        "      subj_id = int(re.findall('\\d+(?=.txt)', file)[0])\n",
        "      if subj_id not in self.patient2label_dict: #icd9 filter\n",
        "        continue\n",
        "\n",
        "      #icd9 process  \n",
        "      code_vec = self.create_patient_label_vec(subj_id, self.patient2label_dict, \n",
        "                                               self.label2idx_dict)\n",
        "      if sum(code_vec) == 0:\n",
        "        continue\n",
        "      codes.append(code_vec)\n",
        "\n",
        "      #cui process\n",
        "      cui_input = self.create_cui_input_sequence(tokens)\n",
        "      cui_inputs.append(cui_input)\n",
        "\n",
        "    return cui_inputs, codes"
      ],
      "metadata": {
        "id": "vNZSYMog1y7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after the first run, we dumped all critical attributes of ICDLoader so we only need to load them later, avoiding processing the giant MIMIC-III dataset again and again.\n",
        "if RUN_LOADER:\n",
        "  loader = ICDLoader(CORPUS_FILE_PATH, CPT_FILE_PATH, DIAGNOSIS_FILE_PATH, PROCEDURES_FILE_PATH, MIN_EXAMPLES_PER_CODE, MIN_TOKEN_FREQ, MAX_TOKENS_IN_FILE)\n",
        "  cui_inputs, codes = loader.run()"
      ],
      "metadata": {
        "id": "QChkvpCd10_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"rb\") as fp:\n",
        "  token2int = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/cui_inputs.pkl\", \"rb\") as fp:   #Pickling\n",
        "  cui_inputs = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/icd9codes.pkl\", \"rb\") as fp:   #Pickling\n",
        "  codes = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/patient2label_dict.pkl\", \"rb\") as fp:   #Pickling\n",
        "  patient2label_dict = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/label2idx_dict.pkl\", \"rb\") as fp:   #Pickling\n",
        "  label2idx_dict = pickle.load(fp)"
      ],
      "metadata": {
        "id": "vh3qv31PDiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RHr-af9XWL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the model"
      ],
      "metadata": {
        "id": "euR2U5ycWNZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#params\n",
        "maxlen = max([len(patient) for patient in cui_inputs])\n",
        "emb_dim = len(token2int)\n",
        "n_class = len(label2idx_dict)"
      ],
      "metadata": {
        "id": "5asmd-8daX_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-JcVtdG6cevL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cui_inputs,codes, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sFdSksIsWSrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x # shape n_sample x padded length\n",
        "        self.y = y # shape n_sample x n classes [[0,1,0,0], [1,1,1,0]]\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        return(self.x[index], self.y[index])"
      ],
      "metadata": {
        "id": "ZN4e24eH7ESQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = customDataset(X_train, y_train)\n",
        "test_dataset = customDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "eYi0vuIRXt4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the paper discussed training 2 versions of the model: (1) with randomly initialized CUI embeddings, (2) with word2vec-pretrained CUI embeddings\n",
        "if USE_PRETRAINED_W2V:\n",
        "  w2v_model = Word2Vec.load(\"drive/MyDrive/mimic-iii/word2vec.model\")"
      ],
      "metadata": {
        "id": "GYOUvu5wcNKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "    n = len(sequences)\n",
        "    x = torch.zeros((n, maxlen), dtype=torch.long).to(device)\n",
        "    \n",
        "    for patient, cuis in enumerate(sequences):\n",
        "      len_cuis = len(cuis)\n",
        "      x[patient][:len_cuis] = torch.tensor(cuis).to(device)\n",
        "      \n",
        "    return x, y\n",
        "\n",
        "def collate_fn_w2v(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "    n = len(sequences)\n",
        "    x = torch.zeros((n, maxlen), dtype=torch.long).to(device)\n",
        "    \n",
        "    for patient, cuis in enumerate(sequences):\n",
        "      len_cuis = len(cuis)\n",
        "      cui_w2v_idx = [w2v_model.wv.vocab[str(token)].index for token in cuis]\n",
        "      x[patient][:len_cuis] = torch.tensor(cui_w2v_idx).to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "jEE-XLdKWm0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_W2V:\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "else:\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_w2v)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_w2v)"
      ],
      "metadata": {
        "id": "pLvc4ost7lCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_representation(nn.Module):\n",
        "  def __init__(self, in_dim, n_diseases, w2v_model=None):\n",
        "    super(NN_representation, self).__init__()\n",
        "    if w2v_model:\n",
        "      weights = torch.FloatTensor(w2v_model.wv.vectors)\n",
        "      self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300).from_pretrained(weights, freeze=False)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300)\n",
        "    self.avg = nn.AdaptiveMaxPool1d(1)\n",
        "    \n",
        "    self.hidden = nn.Linear(300, 1000)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.final = nn.Linear(1000, n_diseases)\n",
        "    self.act2 = nn.Sigmoid()\n",
        "\n",
        "  def get_hidden(self, x):\n",
        "    temp = self.emb(x)\n",
        "    #print(f\"after emb, {temp.shape}\")\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    #print(f\"after permute, {temp.shape}\")\n",
        "    temp = self.avg(temp)\n",
        "    #print(f\"after avg, {temp.shape}\")\n",
        "    temp = temp.squeeze(-1)\n",
        "    #print(f\"after squeeze, {temp.shape}\")\n",
        "    h = self.hidden(temp)\n",
        "    return h\n",
        "\n",
        "  def forward(self, x):\n",
        "    temp = self.emb(x)\n",
        "    #print(f\"after emb, {temp.shape}\")\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    #print(f\"after permute, {temp.shape}\")\n",
        "    temp = self.avg(temp)\n",
        "    #print(f\"after avg, {temp.shape}\")\n",
        "    temp = temp.squeeze(-1)\n",
        "    #print(f\"after squeeze, {temp.shape}\")\n",
        "    temp = self.hidden(temp)\n",
        "    #print(f\"after hidden, {temp.shape}\")\n",
        "    temp = self.act1(temp)\n",
        "    #print(f\"after relu, {temp.shape}\")\n",
        "    temp = self.final(temp)\n",
        "    #print(f\"after linear hidden, {temp.shape}\")\n",
        "    res = self.act2(temp)\n",
        "    return res"
      ],
      "metadata": {
        "id": "RbIUQDSd0TGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not USE_PRETRAINED_W2V:\n",
        "  nnmodel =NN_representation(emb_dim, n_class).to(device)\n",
        "else:\n",
        "  nnmodel =NN_representation(emb_dim, n_class, w2v_model).to(device)\n",
        "optimizer = torch.optim.RMSprop(nnmodel.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tO36rB2kaAyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pzdakpOuWGqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation"
      ],
      "metadata": {
        "id": "CsJbyRccWHR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, n_epochs):\n",
        "  model.train()\n",
        "  for epoch in range(n_epochs):\n",
        "    current_loss = 0\n",
        "    for current_x, current_y in loader:\n",
        "      pred = model(current_x)\n",
        "      loss = criterion(pred, current_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    train_loss = current_loss/len(loader)\n",
        "    print(f\"after epoch {epoch}, the training loss is {train_loss}\")\n"
      ],
      "metadata": {
        "id": "Tf7_D2aaw2rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(nnmodel, train_loader, n_epochs)"
      ],
      "metadata": {
        "id": "kAqZDtAcgdAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, loader):\n",
        "  model.eval()\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for current_x, current_y in loader:\n",
        "      preds = model(current_x).cpu().detach().numpy()\n",
        "      preds_labels = preds>0.5\n",
        "      y_pred.append(preds_labels)\n",
        "      y_true.append(current_y.cpu().numpy())\n",
        "  y_pred = np.vstack(y_pred)  \n",
        "  y_true = np.vstack(y_true)        \n",
        "\n",
        "  p,r,f,_ = precision_recall_fscore_support(y_pred, y_true, average='macro')\n",
        "  acc = accuracy_score(y_pred, y_true)\n",
        "  return p,r,f,acc"
      ],
      "metadata": {
        "id": "mEsfpNvy26K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy = test(nnmodel, test_loader)"
      ],
      "metadata": {
        "id": "LVkdbKlqkbpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy"
      ],
      "metadata": {
        "id": "Irc9lEZUlG5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_train, recall_train, f1_train, accuracy_train = test(nnmodel, train_loader)"
      ],
      "metadata": {
        "id": "bwnoyfB18DRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_train, recall_train, f1_train, accuracy_train"
      ],
      "metadata": {
        "id": "MazX9aY5luaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eEOgJlqlV967"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model for tasks in SVM_Models notebook."
      ],
      "metadata": {
        "id": "ENlRLBn9V_tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(nnmodel.state_dict(), \"drive/MyDrive/mimic-iii/nnmodel_state_dict.pt\")"
      ],
      "metadata": {
        "id": "py99aGDTW4mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G2BxtO-OcBgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n-g2K6q64eTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparamter Tuning"
      ],
      "metadata": {
        "id": "fyuBZXvf4eYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_params = [0.001, 0.0001]\n",
        "n_epochs_params = [50, 75, 100]\n",
        "batch_size_params = [32, 50, 70]\n",
        "param_dict = {}\n",
        "\n",
        "for l in lr_params:\n",
        "  for e in n_epochs_params:\n",
        "    for b in batch_size_params:\n",
        "      train_loader = DataLoader(train_dataset, batch_size=b, shuffle=True, collate_fn=collate_fn)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=b, shuffle=True, collate_fn=collate_fn)\n",
        "      nnmodel =NN_representation(emb_dim, n_class).to(device)\n",
        "      optimizer = torch.optim.RMSprop(nnmodel.parameters(), lr=l)\n",
        "      train(nnmodel, train_loader, e)\n",
        "      precision, recall, f1, accuracy = test(nnmodel, test_loader)\n",
        "      param_dict[(l, e, b)] = (precision, recall, f1, accuracy)\n",
        "      print(param_dict)\n",
        "      print()"
      ],
      "metadata": {
        "id": "dMoMiGAB4iGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "1rSjQNE-7_sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x3N--U2G8iW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}