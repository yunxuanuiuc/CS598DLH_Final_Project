{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXgI_uxrncBm",
        "outputId": "86a6fd1e-df0b-472a-f9b2-0b3817fc68b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUOtofxBnwww"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "import os, os.path\n",
        "import shutil\n",
        "\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A12YZ76F1_Qh"
      },
      "outputs": [],
      "source": [
        "MIN_TOKEN_FREQ = 100\n",
        "MAX_TOKENS_IN_FILE = 10000\n",
        "MIN_EXAMPLES_PER_CODE = 1000\n",
        "TEST_SIZE = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CPT_FILE_PATH = \"drive/MyDrive/mimic-iii/CPTEVENTS.csv\"\n",
        "DIAGNOSIS_FILE_PATH = \"drive/MyDrive/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "PROCEDURES_FILE_PATH = \"drive/MyDrive/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "CORPUS_FILE_PATH = \"drive/MyDrive/mimic-iii/cuis\" "
      ],
      "metadata": {
        "id": "baKiDjbNo6Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubMJzXUh7RuG"
      },
      "source": [
        "# Load the MIMIC-III data and pretrain word embeddings\n",
        "This first cell in this section is the same as the ICDLoader in NNModels.ipynb. In colab env, it's challenging to import classes from other ipynb files - apologies for the duplicate code. We use this class in this notebook to pretrain a word2vec and a doc2vec model, which will be used later in NNModels.ipynb and SVM_Models.ipynb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASiXkDC-sl4J"
      },
      "outputs": [],
      "source": [
        "class ICDLoader: \n",
        "  \"\"\"Load ICD billing codes labels for each patient\"\"\"\n",
        "\n",
        "  def __init__(self, corpus_file_path, cpt_file_path, diagnosis_file_path, procedures_file_path, min_examples_per_code, min_token_freq, max_tokens_in_file):\n",
        "    self.corpus_path = corpus_file_path\n",
        "    self.cpt_path = cpt_file_path\n",
        "    self.diagnosis_path = diagnosis_file_path\n",
        "    self.procedures_path = procedures_file_path\n",
        "   \n",
        "    self.max_tokens_in_file = max_tokens_in_file\n",
        "    self.min_examples_per_code = min_examples_per_code\n",
        "    self.min_token_freq = min_token_freq\n",
        "    self.patient2label_dict = None #mapping from patient -> ICD9 label codes\n",
        "    self.label2idx_dict = None #mapping from ICD9 label code -> embedding idx\n",
        "\n",
        "    self.token2int = {}\n",
        "\n",
        "  def df_make_code(self, df, icd_type, code_len, code_col):\n",
        "    #making special short string for codes\n",
        "    df['short_code'] = icd_type + \"_\" + df[code_col].astype(str).str[:code_len]\n",
        "    return df\n",
        "  \n",
        "  def get_label2freq_df(self, df, min_examples_per_code):\n",
        "    #get a filtered label codes -> frequency mapping table \n",
        "    label2freq_df = df[[\"SUBJECT_ID\", \"short_code\"]].groupby(\"short_code\").nunique()\n",
        "    label2freq_df = label2freq_df[label2freq_df[\"SUBJECT_ID\"]>min_examples_per_code]\n",
        "    label2freq_df.rename({\"SUBJECT_ID\": \"freq\"}, axis=1, inplace=True)\n",
        "    return label2freq_df.reset_index()\n",
        "\n",
        "  def get_patient2label_df(self, df, label2freq_df):\n",
        "    #get df of patient mapping to all their filtered ICD9 code labels (filtered by freq)\n",
        "    df_filtered = df[df[\"short_code\"].isin(label2freq_df[\"short_code\"])] \n",
        "    patient2label_df = df_filtered[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "                          .groupby(\"SUBJECT_ID\")\\\n",
        "                          .agg({'short_code':lambda sf: set(sf)})\n",
        "    patient2label_df.rename({\"short_code\":\"short_codes\"}, axis=1, inplace=True)          \n",
        "    return patient2label_df.reset_index()\n",
        "\n",
        "  def create_patient_label_vec(self, subj_id, patient2label_dict, label2idx_dict):\n",
        "    #make patient label vector\n",
        "    code_vec = [0]*len(label2idx_dict)\n",
        "    codes = patient2label_dict[subj_id]\n",
        "    for code in codes:\n",
        "      code_vec[label2idx_dict[code]] = 1\n",
        "    return code_vec\n",
        "\n",
        "  def make_cui_token2int_mapping(self):\n",
        "    #count tokens\n",
        "    i=0\n",
        "    token_count_dict = {}\n",
        "    for file in os.listdir(self.corpus_path):\n",
        "      text = open(os.path.join(self.corpus_path,file)).read()\n",
        "      tokens = [token for token in text.split()] #assume all cui in file splitted by space\n",
        "      if len(tokens) > self.max_tokens_in_file:\n",
        "        continue\n",
        "      else:\n",
        "        for token in tokens:\n",
        "          if token in token_count_dict:\n",
        "            token_count_dict[token] += 1\n",
        "          else:\n",
        "            token_count_dict[token] = 1\n",
        "    \n",
        "    #make token2int mapping\n",
        "    oov_idx = 0\n",
        "    idx = 1\n",
        "    self.token2int['oov_word'] = 0\n",
        "    for token, count in token_count_dict.items():\n",
        "      if count > self.min_token_freq:\n",
        "        self.token2int[token] = idx\n",
        "        idx += 1\n",
        "  \n",
        "  def create_cui_input_sequence(self, tokens):\n",
        "    #create cui_input_sequence from cui tokens\n",
        "    input = []\n",
        "    tokens_set = set(tokens)\n",
        "\n",
        "    for token in tokens_set:\n",
        "      if token in self.token2int:\n",
        "        input.append(self.token2int[token])\n",
        "      else:\n",
        "        input.append(self.token2int['oov_word'])\n",
        "\n",
        "    return input\n",
        "\n",
        "  def run(self):\n",
        "    #run everything\n",
        "\n",
        "    #codes init\n",
        "    cpt = pd.read_csv(self.cpt_path)\n",
        "    diagnosis = pd.read_csv(self.diagnosis_path)\n",
        "    procedures = pd.read_csv(self.procedures_path)\n",
        "\n",
        "    #codes init\n",
        "    cpt = self.df_make_code(cpt, 'cpt', 5, 'CPT_NUMBER')\n",
        "    diagnosis = self.df_make_code(diagnosis, 'diag', 3, 'ICD9_CODE')\n",
        "    procedures = self.df_make_code(procedures, 'proc', 2, 'ICD9_CODE')\n",
        "\n",
        "    #codes init\n",
        "    all_codes = cpt[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "              .append(diagnosis[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\\\n",
        "              .append(procedures[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\n",
        "\n",
        "    #codes init\n",
        "    label2freq_df = self.get_label2freq_df(all_codes, self.min_examples_per_code)\n",
        "    patient2label_df = self.get_patient2label_df(all_codes, label2freq_df)\n",
        "    \n",
        "    #codes init\n",
        "    self.label2idx_dict = dict(label2freq_df.reset_index()[[\"short_code\", \"index\"]].values)\n",
        "    self.patient2label_dict = dict(patient2label_df.values)\n",
        "\n",
        "    #cui init\n",
        "    self.make_cui_token2int_mapping()\n",
        "\n",
        "    codes = []\n",
        "    cui_inputs = []\n",
        "\n",
        "    #processing\n",
        "    for file in os.listdir(self.corpus_path): #list files to run\n",
        "      text = open(os.path.join(self.corpus_path, file)).read()\n",
        "      tokens = [token for token in text.split()]\n",
        "      if len(tokens) > self.max_tokens_in_file: #cui filter\n",
        "        continue\n",
        "\n",
        "      subj_id = int(re.findall('\\d+(?=.txt)', file)[0])\n",
        "      if subj_id not in self.patient2label_dict: #icd9 filter\n",
        "        continue\n",
        "\n",
        "      #icd9 process  \n",
        "      code_vec = self.create_patient_label_vec(subj_id, self.patient2label_dict, \n",
        "                                               self.label2idx_dict)\n",
        "      if sum(code_vec) == 0:\n",
        "        continue\n",
        "      codes.append(code_vec)\n",
        "\n",
        "      #cui process\n",
        "      cui_input = self.create_cui_input_sequence(tokens)\n",
        "      cui_inputs.append(cui_input)\n",
        "\n",
        "\n",
        "    return cui_inputs, codes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = ICDLoader(CORPUS_FILE_PATH, CPT_FILE_PATH, DIAGNOSIS_FILE_PATH, PROCEDURES_FILE_PATH, MIN_EXAMPLES_PER_CODE, MIN_TOKEN_FREQ, MAX_TOKENS_IN_FILE)\n",
        "cui_inputs, codes = loader.run()"
      ],
      "metadata": {
        "id": "FcYT2y4QpLDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving dictionary to avoid processing 46k+ files everytime\n",
        "save_dict_file = open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"wb\")\n",
        "pickle.dump(loader.token2int, save_dict_file)\n",
        "save_dict_file.close()\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"rb\") as fp:\n",
        "  token2int_dict = pickle.load(fp)"
      ],
      "metadata": {
        "id": "1rr6Yr1LNnrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"drive/MyDrive/mimic-iii/patient2label_dict.pkl\", \"wb\") as fp:   #Pickling\n",
        "#   pickle.dump(loader.patient2label_dict, fp)\n",
        "\n",
        "# with open(\"drive/MyDrive/mimic-iii/label2idx_dict.pkl\", \"wb\") as fp:   #Pickling\n",
        "#   pickle.dump(loader.label2idx_dict, fp)\n",
        "\n",
        "# with open(\"drive/MyDrive/mimic-iii/cui_inputs.pkl\", \"wb\") as fp:   #Pickling\n",
        "#   pickle.dump(cui_inputs, fp)\n",
        "\n",
        "# with open(\"drive/MyDrive/mimic-iii/icd9codes.pkl\", \"wb\") as fp:   #Pickling\n",
        "#   pickle.dump(codes, fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/cui_inputs.pkl\", \"rb\") as fp:   #Pickling\n",
        "  cui_inputs = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/icd9codes.pkl\", \"rb\") as fp:   #Pickling\n",
        "  codes = pickle.load(fp)"
      ],
      "metadata": {
        "id": "iqxIDhDsORYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cui_inputs), len(codes)"
      ],
      "metadata": {
        "id": "DCTtxKrUz6mX",
        "outputId": "154b2ae9-587b-4938-d958-79a336d34de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41617, 41617)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec Model"
      ],
      "metadata": {
        "id": "bgCLQ1BNUsr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cui_inputs_str = [[str(x) for x in cui_input] for cui_input in cui_inputs]"
      ],
      "metadata": {
        "id": "6yainOkRACbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cui_inputs_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGWk6qu6A4w9",
        "outputId": "661f1c80-52d5-461e-adab-56d991065ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41617"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=cui_inputs_str, window=5, size=300, workers=4, min_count=1)"
      ],
      "metadata": {
        "id": "OS2LwZFsUw_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdDLFjgZpXPH",
        "outputId": "2aea473f-0852-453c-ffb2-47f77a873f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.5117495e-01, -1.5825114e-01, -2.8098634e-01, ...,\n",
              "        -4.3912478e-02,  2.5941911e-01,  6.2685108e-01],\n",
              "       [-7.8855312e-01, -2.0701051e+00,  6.7504030e-01, ...,\n",
              "        -4.8524055e+00, -1.4103217e+00,  2.7832153e+00],\n",
              "       [ 4.0319734e+00, -1.8833885e+00, -1.8876145e+00, ...,\n",
              "         5.8676332e-01,  5.2643979e-01,  5.5151582e+00],\n",
              "       ...,\n",
              "       [ 6.0104658e-03,  7.8745018e-04, -4.4967472e-03, ...,\n",
              "        -6.9574029e-03, -1.3573231e-02,  1.0548139e-02],\n",
              "       [ 1.5774533e-02,  1.0217195e-02, -9.9627888e-03, ...,\n",
              "        -1.8214449e-02, -3.1583119e-02,  7.8437198e-03],\n",
              "       [-6.6564404e-03, -1.4135306e-02,  1.6191775e-02, ...,\n",
              "         2.9815650e-02,  2.6409375e-02, -5.4407003e-03]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"drive/MyDrive/mimic-iii/word2vec_2.model\")"
      ],
      "metadata": {
        "id": "K8IkMa1O_2VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ot5TGFsaDESo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec model"
      ],
      "metadata": {
        "id": "FbPwYEKODEyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(cui_inputs_str)]\n",
        "model = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "qDTl2l_tFbdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"drive/MyDrive/MIMIC/mimic-iii/doc2vec.model\")"
      ],
      "metadata": {
        "id": "mUkBBnPIHEWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdSMqmPZPnCi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MIMICIII_pretrain_embeddings.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}