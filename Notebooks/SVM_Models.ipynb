{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_Models.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Qm_hCPox-r",
        "outputId": "6fcf3733-2037-4c08-a4a2-966b4ce418db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Obsesity Challenge data: Feeding embeddings into SVM model\n",
        "\n",
        "In this notebook, we replicated what the paper discussed in 2.2 (partially) and 2.3:\n",
        "1. Processing Obesity Challenge data\n",
        "2. Models. This includes\n",
        "   *  Construct a sparse baseline SVM model using bag-of-words as features\n",
        "   *  Construct a second baseline SVM model using SVD vectors as features\n",
        "   *  Construct the proposed SVM model, with vectors from the hidden layer of the previously trained NN model as features\n",
        "   * Ablation study I: using vectors from the averaged layer of the NN model as features\n",
        "   * Ablation study II: using Word2Vec embeddings directly as features\n",
        "   * Ablation study III: using Doc2Vec embeddings directly as features\n",
        "3. Evaluation of model performances\n",
        "   * Standard Evaluation\n",
        "   * Filtering out diseases with few positive labels, then evaluate \n"
      ],
      "metadata": {
        "id": "asKhTOwHLkRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "#https://docs.python.org/3/library/xml.etree.elementtree.html\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "kGQoiTBpo3Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "YuJKhsBtk0LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_annotation_path='/content/drive/My Drive/Obesity challenge/obesity_standoff_annotations_training.xml'\n",
        "train_annotation_path2='/content/drive/My Drive/Obesity challenge/obesity_standoff_annotations_training_addendum3.xml'\n",
        "cui_path='/content/drive/My Drive/Obesity challenge/train_data_cuis/'\n",
        "\n",
        "#load cui2token dict from MIMIC data\n",
        "with open(\"drive/MyDrive/mimic-iii/token2int_dict.pkl\", \"rb\") as fp:\n",
        "  mimic_cui2token = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/label2idx_dict.pkl\", \"rb\") as fp:   #Pickling\n",
        "  label2idx_dict = pickle.load(fp)\n",
        "\n",
        "with open(\"drive/MyDrive/mimic-iii/cui_inputs.pkl\", \"rb\") as fp:   #Pickling\n",
        "  cui_inputs = pickle.load(fp)"
      ],
      "metadata": {
        "id": "TzG1L0e-onMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S6Dg3MpFNomj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Obesity Challenge Data Processor"
      ],
      "metadata": {
        "id": "5pCbQjl_NpQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class obesityDataLoader:\n",
        "  def __init__(self, annotation_path, annotation_path2, cui_path):\n",
        "    # obesity data has 2 train files, so we need to read both of them\n",
        "    self.train_annotation=[ET.parse(annotation_path), ET.parse(annotation_path2)] # xml tree containing labels for all patients\n",
        "    self.cui_path=cui_path # dir of txts containing cuis for each patient\n",
        "    self.diseases=self.get_diseases()\n",
        "    self.diseases2int={self.diseases[i]:i for i in range(len(self.diseases))}\n",
        "    self.label2int= {'Y':0, 'N':1, 'Q':2, 'U':3}\n",
        "    self.doc2labels=[]\n",
        "    self.token2int={}\n",
        "\n",
        "  def get_diseases(self):\n",
        "    diseases = []\n",
        "    for train_annotation in self.train_annotation:\n",
        "      for disease in train_annotation.iter('disease'):\n",
        "        name = disease.attrib['name']\n",
        "        diseases.append(name)\n",
        "    diseases = list(set(diseases))\n",
        "    diseases.sort(reverse = False ) \n",
        "    return diseases\n",
        "\n",
        "  def map_annotation_to_labels(self):\n",
        "    patient_results = {}\n",
        "    for annotation_root in self.train_annotation:\n",
        "      root = annotation_root.getroot()\n",
        "      root_intuitive = root[0] # only intuitive tasks are needed in the paper\n",
        "      for disease in root_intuitive.iter('disease'):\n",
        "        dis_name = disease.attrib[\"name\"]\n",
        "        for patient in disease.iter('doc'):\n",
        "          id = int(patient.attrib['id']) # use int id\n",
        "          if id not in patient_results:\n",
        "            patient_results[id] = [0] * len(self.diseases)\n",
        "          res = patient.attrib['judgment']\n",
        "          int_label = self.label2int[res]\n",
        "          disease_id = self.diseases2int[dis_name]\n",
        "          patient_results[id][disease_id] = int_label\n",
        "    return patient_results   # this contains # patients x # possible diseases, dictionary: id to list of diseases   \n",
        "\n",
        "  def set_cuis(self, mimic_cui_dict):   \n",
        "      self.token2int= mimic_cui_dict\n",
        "      \n",
        "  def create_cui_input_sequence(self, tokens):\n",
        "    input = []\n",
        "    tokens_set = set(tokens)\n",
        "    for token in tokens_set:\n",
        "      if token in self.token2int:\n",
        "        input.append(self.token2int[token])\n",
        "      else:\n",
        "        input.append(self.token2int['oov_word'])\n",
        "    return input\n",
        "  \n",
        "  def map_annotation_to_labels_test(self, annotation_root):\n",
        "      patient_results = {}\n",
        "      root = annotation_root.getroot()\n",
        "      root_intuitive = root[0] # only intuitive tasks are needed in the paper\n",
        "      for disease in root_intuitive.iter('disease'):\n",
        "        dis_name = disease.attrib[\"name\"]\n",
        "        for patient in disease.iter('doc'):\n",
        "          id = int(patient.attrib['id']) # use int id\n",
        "          if id not in patient_results:\n",
        "            patient_results[id] = [0] * len(self.diseases)\n",
        "          res = patient.attrib['judgment']\n",
        "          int_label = self.label2int[res]\n",
        "          disease_id = self.diseases2int[dis_name]\n",
        "          patient_results[id][disease_id] = int_label\n",
        "      return patient_results   # this contains # patients x # possible diseases, dictionary: id to list of diseases \n",
        "\n",
        "  def get_test_data(self, test_annotation_path, test_cui_path, raw=False):\n",
        "    test_annotation = ET.parse(test_annotation_path)\n",
        "    codes = []\n",
        "    labels = []\n",
        "    test_patient_labels = self.map_annotation_to_labels_test(test_annotation)\n",
        "    for file in os.listdir(test_cui_path):\n",
        "      text = open(os.path.join(test_cui_path, file)).read()     \n",
        "      subj_id = int(re.findall('\\d+(?=.txt)', file)[0])\n",
        "\n",
        "      if raw:\n",
        "        cui_tokens = text\n",
        "      else:\n",
        "        cuis = [token for token in text.split()]\n",
        "        cui_tokens = self.create_cui_input_sequence(cuis)\n",
        "      \n",
        "      if subj_id in test_patient_labels:\n",
        "        codes.append(cui_tokens)\n",
        "        labels.append(test_patient_labels[subj_id])  \n",
        "          \n",
        "    diseases_label_dict ={}\n",
        "    for disease in self.diseases:\n",
        "      diseases_label_dict[disease] = [patient[self.diseases2int[disease]] for patient in labels]\n",
        "    return codes, diseases_label_dict\n",
        "\n",
        "  def get_train_data(self, raw=False):\n",
        "    codes = []\n",
        "    labels = []\n",
        "    all_patient_labels = self.map_annotation_to_labels()\n",
        "\n",
        "    for file in os.listdir(self.cui_path):\n",
        "      text = open(os.path.join(self.cui_path, file)).read()\n",
        "      subj_id = int(re.findall('\\d+(?=.txt)', file)[0])\n",
        "      if raw:\n",
        "        cui_tokens = text\n",
        "      else:\n",
        "        cuis = [token for token in text.split()]\n",
        "        cui_tokens = self.create_cui_input_sequence(cuis)\n",
        "      if subj_id in all_patient_labels:\n",
        "        codes.append(cui_tokens)\n",
        "        labels.append(all_patient_labels[subj_id])\n",
        "    diseases_label_dict ={}\n",
        "    for disease in self.diseases:\n",
        "      diseases_label_dict[disease] = [patient[self.diseases2int[disease]] for patient in labels]\n",
        "    return codes, diseases_label_dict\n",
        "    # diseases_label_dict: {\"Asthma\":[1,0,1,0,....], \"CAD\":[0,0,1,1,....], ...}"
      ],
      "metadata": {
        "id": "yqVSP2IkpB4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = obesityDataLoader(train_annotation_path, train_annotation_path2, cui_path)\n",
        "train_data_loader.set_cuis(mimic_cui2token)"
      ],
      "metadata": {
        "id": "ko0OcyrGpChU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cui_path=\"/content/drive/My Drive/Obesity challenge/test_data_cuis/\"\n",
        "test_annotation_path='/content/drive/My Drive/Obesity challenge/obesity_standoff_annotations_test.xml'"
      ],
      "metadata": {
        "id": "OvkLFZ4Qyhm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bEbcLoToNtUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Models"
      ],
      "metadata": {
        "id": "TL-szrnGNtlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Sparse baseline Model"
      ],
      "metadata": {
        "id": "dPVbGfj6khPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tfidf features\n",
        "def count_and_tfidf(count_vectorizer, tfidf, data, test=False):\n",
        "  if not test:\n",
        "    data_count_matrix = count_vectorizer.fit_transform(data)\n",
        "    data_tfidf = tfidf.fit_transform(data_count_matrix)\n",
        "  else:\n",
        "    data_count_matrix = count_vectorizer.transform(data)\n",
        "    data_tfidf = tfidf.transform(data_count_matrix)\n",
        "  return data_tfidf\n",
        "\n",
        "#evaluation\n",
        "def eval_p_r_and_f1(pred, y_test):\n",
        "  p = precision_score(y_test, pred, average='macro')\n",
        "  r = recall_score(y_test, pred, average='macro')\n",
        "  f1 = f1_score(y_test, pred, average='macro')\n",
        "  return p, r, f1\n",
        "\n",
        "#run train and predict\n",
        "def run_sparse_baseline_model(loader, disease, test_annotation_path, test_cui_path):\n",
        "  x_train, y_train_dict = loader.get_train_data(raw=True) \n",
        "  y_train = y_train_dict[disease]\n",
        "  x_test, y_test_dict = loader.get_test_data(test_annotation_path, test_cui_path, raw=True) #TODO\n",
        "  y_test = y_test_dict[disease]\n",
        "\n",
        "  count_vectorizer = CountVectorizer(\n",
        "    ngram_range=(1,1),\n",
        "    stop_words='english',\n",
        "    min_df=0,\n",
        "    vocabulary=None,\n",
        "    binary=False)\n",
        "\n",
        "  tfidf = TfidfTransformer()\n",
        "\n",
        "  train_tfidf = count_and_tfidf(count_vectorizer, tfidf, x_train)\n",
        "  test_tfidf = count_and_tfidf(count_vectorizer, tfidf, x_test, test=True)\n",
        "\n",
        "  clf = LinearSVC(class_weight='balanced')\n",
        "  clf.fit(train_tfidf, y_train)\n",
        "  pred = clf.predict(test_tfidf)\n",
        "\n",
        "  p, r, f1 = eval_p_r_and_f1(pred, y_test)\n",
        "  return p, r, f1"
      ],
      "metadata": {
        "id": "wogHVRt_pNXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1 = run_sparse_baseline_model(train_data_loader, \"Asthma\", test_annotation_path, test_cui_path)\n",
        "print(p, r, f1)"
      ],
      "metadata": {
        "id": "MoDCdtTIp1qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f285696-83f3-4f90-ed37-9648d7704174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7553191489361701 0.6993355481727574 0.7185185185185186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DnDPfvJCI8ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 SVD Baseline Model"
      ],
      "metadata": {
        "id": "tCP_0aciIzDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SVD has same pipeline as sparse feature SVM\n",
        "def run_svd_baseline_model(loader, disease, test_annotation_path, test_cui_path):\n",
        "    x_train, y_train_dict = loader.get_train_data(raw=True) \n",
        "    y_train = y_train_dict[disease]\n",
        "    x_test, y_test_dict = loader.get_test_data(test_annotation_path, test_cui_path, raw=True) #TODO\n",
        "    y_test = y_test_dict[disease]\n",
        "\n",
        "    count_vectorizer = CountVectorizer(\n",
        "    ngram_range=(1,1),\n",
        "    stop_words='english',\n",
        "    min_df=0,\n",
        "    vocabulary=None,\n",
        "    binary=False)\n",
        "\n",
        "    tfidf = TfidfTransformer()\n",
        "\n",
        "    train_tfidf = count_and_tfidf(count_vectorizer, tfidf, x_train)\n",
        "    test_tfidf = count_and_tfidf(count_vectorizer, tfidf, x_test, test=True)\n",
        "\n",
        "    svd = TruncatedSVD(n_components=300)\n",
        "    svd_train_tfidf = svd.fit_transform(train_tfidf)\n",
        "    svd_test_tfidf = svd.transform(test_tfidf)\n",
        "\n",
        "    svd_clf = LinearSVC(class_weight='balanced')\n",
        "    svd_clf.fit(svd_train_tfidf, y_train)\n",
        "    svd_pred = svd_clf.predict(svd_test_tfidf)\n",
        "\n",
        "    p, r, f1 = eval_p_r_and_f1(svd_pred, y_test)\n",
        "    return p, r, f1"
      ],
      "metadata": {
        "id": "ZWt0a144qb-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1 = run_svd_baseline_model(train_data_loader, \"Asthma\", test_annotation_path, test_cui_path)\n",
        "print(p, r, f1)"
      ],
      "metadata": {
        "id": "G1EecW49rDY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3916bc8e-0ad0-4176-948a-4de4e3b86260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7226704316256556 0.703765227021041 0.7119410887038552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Dense Neural Network Model"
      ],
      "metadata": {
        "id": "xNMD_QQlMJF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model configs\n",
        "criterion = nn.BCELoss()\n",
        "n_epochs = 75\n",
        "batch_size = 50\n",
        "\n",
        "maxlen = max([len(patient) for patient in cui_inputs])\n",
        "emb_dim = len(mimic_cui2token)\n",
        "n_class = len(label2idx_dict)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "HmQpycZrjZb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x # shape n_sample x padded length\n",
        "        self.y = y # shape n_sample x n classes [[0,1,0,0], [1,1,1,0]]\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        return(self.x[index], self.y[index])\n",
        "\n",
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "\n",
        "    n = len(sequences)\n",
        "    y = torch.zeros((n, 100), dtype=torch.float).to(device)\n",
        "\n",
        "    x = torch.zeros((n, maxlen), dtype=torch.long).to(device)\n",
        "    \n",
        "    for patient, cuis in enumerate(sequences):\n",
        "      len_cuis = len(cuis)\n",
        "      if len_cuis > maxlen:\n",
        "        cuis = cuis[:maxlen]\n",
        "        x[patient][:maxlen] = torch.tensor(cuis).to(device)\n",
        "      else:\n",
        "        x[patient][:len_cuis] = torch.tensor(cuis).to(device)\n",
        "    return x, y\n",
        "    \n",
        "def get_hidden_states(loader, model):\n",
        "    model.eval()\n",
        "    hiddens_lst = []\n",
        "    for current_x, current_y in loader:\n",
        "      hiddens = model.get_hidden(current_x).cpu().detach().numpy()\n",
        "      hiddens_lst.append(hiddens)\n",
        "    hiddens_lst = np.vstack(hiddens_lst)\n",
        "    return hiddens_lst #output should be 2d list of hidden states\n",
        "\n",
        "def get_averaged_states(loader, model):\n",
        "    model.eval()\n",
        "    avg_lst = []\n",
        "    for current_x, current_y in loader:\n",
        "      hiddens = model.get_averaged(current_x).cpu().detach().numpy()\n",
        "      avg_lst.append(hiddens)\n",
        "    avg_lst = np.vstack(avg_lst)\n",
        "    return avg_lst \n"
      ],
      "metadata": {
        "id": "MwV4sTlYtvzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_representation(nn.Module):\n",
        "  def __init__(self, in_dim, n_diseases, w2v_model=None):\n",
        "    super(NN_representation, self).__init__()\n",
        "    if w2v_model:\n",
        "      weights = torch.FloatTensor(w2v_model.wv.vectors)\n",
        "      self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300).from_pretrained(weights)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300)\n",
        "    self.avg = nn.AdaptiveMaxPool1d(1)\n",
        "    self.hidden = nn.Linear(300, 1000)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.final = nn.Linear(1000, n_diseases)\n",
        "    self.act2 = nn.Sigmoid()\n",
        "\n",
        "  def get_hidden(self, x):\n",
        "    temp = self.emb(x)\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    temp = self.avg(temp)\n",
        "    temp = temp.squeeze(-1)\n",
        "    h = self.hidden(temp)\n",
        "    return h\n",
        "\n",
        "  def get_averaged(self, x):\n",
        "    temp = self.emb(x)\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    temp = self.avg(temp)\n",
        "    temp = temp.squeeze(-1)\n",
        "    return temp\n",
        "\n",
        "  def forward(self, x):\n",
        "    temp = self.emb(x)\n",
        "    #print(f\"after emb, {temp.shape}\")\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    #print(f\"after permute, {temp.shape}\")\n",
        "    temp = self.avg(temp)\n",
        "    #print(f\"after avg, {temp.shape}\")\n",
        "    temp = temp.squeeze(-1)\n",
        "    #print(f\"after squeeze, {temp.shape}\")\n",
        "    temp = self.hidden(temp)\n",
        "    #print(f\"after hidden, {temp.shape}\")\n",
        "    temp = self.act1(temp)\n",
        "    #print(f\"after relu, {temp.shape}\")\n",
        "    temp = self.final(temp)\n",
        "    #print(f\"after linear hidden, {temp.shape}\")\n",
        "    res = self.act2(temp)\n",
        "    return res"
      ],
      "metadata": {
        "id": "MifiMkz7mhze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run nn model and feed into svm\n",
        "def run_dense_model(loader, disease, test_annotation_path, test_cui_path, model):\n",
        "    x_train, y_train_dict = loader.get_train_data(raw=False) \n",
        "    y_train = y_train_dict[disease]\n",
        "    x_test, y_test_dict = loader.get_test_data(test_annotation_path, test_cui_path, raw=False) #TODO\n",
        "    y_test = y_test_dict[disease]\n",
        "\n",
        "    # model configs\n",
        "    batch_size = 50\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_dataset = customDataset(x_train, y_train)\n",
        "    test_dataset = customDataset(x_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    train_hiddens = get_hidden_states(train_loader, model)\n",
        "    test_hiddens = get_hidden_states(test_loader, model)\n",
        "\n",
        "    dense_clf = LinearSVC(class_weight='balanced')\n",
        "    dense_clf.fit(train_hiddens, y_train)\n",
        "    dense_pred = dense_clf.predict(test_hiddens)\n",
        "\n",
        "    p, r, f1 = eval_p_r_and_f1(dense_pred, y_test)\n",
        "    return p, r, f1"
      ],
      "metadata": {
        "id": "mW6Wd40WtukK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =NN_representation(emb_dim, n_class).to(device)\n",
        "# load model pretrained in another NNModels notebook\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/mimic-iii/nnmodel_tuned_state_dict.pt\"))\n",
        "p, r, f1 = run_dense_model(train_data_loader, \"Asthma\", test_annotation_path, test_cui_path, model)\n",
        "print(p, r, f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGYVKfZQoGUe",
        "outputId": "48c1d104-c191-4393-c956-57085e87109b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7718075539568345 0.7231450719822813 0.7413271480435659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vS8eiSN16Ett"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Ablation Study I: Neural Network Model with vectors from averaged layer as embeddings"
      ],
      "metadata": {
        "id": "SpEpLY0I6FRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dense_model_avg_states(loader, disease, test_annotation_path, test_cui_path, model):\n",
        "    x_train, y_train_dict = loader.get_train_data(raw=False) \n",
        "    y_train = y_train_dict[disease]\n",
        "    x_test, y_test_dict = loader.get_test_data(test_annotation_path, test_cui_path, raw=False) #TODO\n",
        "    y_test = y_test_dict[disease]\n",
        "\n",
        "    # model configs\n",
        "    batch_size = 50\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_dataset = customDataset(x_train, y_train)\n",
        "    test_dataset = customDataset(x_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    train_hiddens = get_averaged_states(train_loader, model)\n",
        "    test_hiddens = get_averaged_states(test_loader, model)\n",
        "\n",
        "    dense_clf = LinearSVC(class_weight='balanced')\n",
        "    dense_clf.fit(train_hiddens, y_train)\n",
        "    dense_pred = dense_clf.predict(test_hiddens)\n",
        "\n",
        "    p, r, f1 = eval_p_r_and_f1(dense_pred, y_test)\n",
        "    return p, r, f1"
      ],
      "metadata": {
        "id": "ItdOXcty601N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =NN_representation(emb_dim, n_class).to(device)\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/mimic-iii/nnmodel_state_dict.pt\"))\n",
        "p, r, f1 = run_dense_model_avg_states(train_data_loader, \"Asthma\", test_annotation_path, test_cui_path, model)\n",
        "print(p, r, f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elo_xQdL606d",
        "outputId": "0c9f3702-3433-477c-90a9-dc1e6b5030ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.886988011988012 0.7859911406423035 0.8203781512605042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "90tUQczp60_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Ablation Study II: Word2Vec Embeddings into SVM Model"
      ],
      "metadata": {
        "id": "duWJnZze7Vju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_vector(data, model):\n",
        "  res=[]\n",
        "  for doc in data:\n",
        "    temp = [model.wv[str(token)] for token in doc]\n",
        "    res.append(sum(temp)/len(temp))  \n",
        "  return np.asarray(res)"
      ],
      "metadata": {
        "id": "whVdLkgU7ek9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained word2vec model (trained in the other notebook)\n",
        "W2Vmodel = Word2Vec.load(\"drive/MyDrive/mimic-iii/word2vec.model\")"
      ],
      "metadata": {
        "id": "HTPWA6_KDWXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_w2v_embeddings_model(loader, disease, test_annotation_path, test_cui_path, model):\n",
        "    x_train, y_train_dict = loader.get_train_data(raw=False) \n",
        "    y_train = y_train_dict[disease]\n",
        "    x_test, y_test_dict = loader.get_test_data(test_annotation_path, test_cui_path, raw=False) #TODO\n",
        "    y_test = y_test_dict[disease]\n",
        "\n",
        "    train_embeddings = get_embeddings_vector(x_train, model)\n",
        "    test_embeddings = get_embeddings_vector(x_test, model)\n",
        "\n",
        "    w2v_clf = LinearSVC(class_weight='balanced')\n",
        "    w2v_clf.fit(train_embeddings, y_train)\n",
        "    w2v_pred = w2v_clf.predict(test_embeddings)\n",
        "\n",
        "    p, r, f1 = eval_p_r_and_f1(w2v_pred, y_test)\n",
        "    return p, r, f1"
      ],
      "metadata": {
        "id": "45JZaGve-7yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P6TWuNWYA4PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Ablation Study III: Doc2Vec Embeddings into SVM Model"
      ],
      "metadata": {
        "id": "7VKLViNYA54_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_d2v_embeddings_vector(data, model):\n",
        "  res=[]\n",
        "  for doc in data:\n",
        "    temp = [str(i) for i in doc]\n",
        "    res.append(model.infer_vector(temp))  \n",
        "  return np.asarray(res)"
      ],
      "metadata": {
        "id": "8OLfqP0sA4VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained doc2vec model (trained in the other notebook)\n",
        "D2Vmodel = Doc2Vec.load(\"drive/MyDrive/mimic-iii/doc2vec.model\")"
      ],
      "metadata": {
        "id": "HQbBIpigB-on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_d2v_embeddings_model(loader, disease, test_annotation_path, test_cui_path, model):\n",
        "    x_train, y_train_dict = loader.get_train_data(raw=False) \n",
        "    y_train = y_train_dict[disease]\n",
        "    x_test, y_test_dict = loader.get_test_data(test_annotation_path, test_cui_path, raw=False) #TODO\n",
        "    y_test = y_test_dict[disease]\n",
        "\n",
        "    train_embeddings = get_d2v_embeddings_vector(x_train, model)\n",
        "    test_embeddings = get_d2v_embeddings_vector(x_test, model)\n",
        "\n",
        "    w2v_clf = LinearSVC(class_weight='balanced')\n",
        "    w2v_clf.fit(train_embeddings, y_train)\n",
        "    w2v_pred = w2v_clf.predict(test_embeddings)\n",
        "\n",
        "    p, r, f1 = eval_p_r_and_f1(w2v_pred, y_test)\n",
        "    return p, r, f1"
      ],
      "metadata": {
        "id": "pHye3RTADViO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1 = run_d2v_embeddings_model(train_data_loader, \"Asthma\", test_annotation_path, test_cui_path, D2Vmodel)\n",
        "print(p, r, f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_HJRvdRDTfu",
        "outputId": "6a5e3314-3c89-47a7-ffe3-e267afb643ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6730769230769231 0.6644518272425248 0.6683695189442316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jwx0Zh7uLYDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Evaluation:"
      ],
      "metadata": {
        "id": "QU8ueFHzLZ85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Standard Evaluation"
      ],
      "metadata": {
        "id": "m9N_UY5XOFKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run evaluation on all diseases for all models\n",
        "\n",
        "sparse_baseline_p = []\n",
        "sparse_baseline_r = []\n",
        "sparse_baseline_f = []\n",
        "\n",
        "sparse_svd_p = []\n",
        "sparse_svd_r = []\n",
        "sparse_svd_f = []\n",
        "\n",
        "dense_p = []\n",
        "dense_r = []\n",
        "dense_f = []\n",
        "\n",
        "dense_avg_p = []\n",
        "dense_avg_r = []\n",
        "dense_avg_f = []\n",
        "\n",
        "w2v_p = []\n",
        "w2v_r = []\n",
        "w2v_f = []\n",
        "\n",
        "d2v_p = []\n",
        "d2v_r = []\n",
        "d2v_f = []\n",
        "\n",
        "model =NN_representation(emb_dim, n_class).to(device)\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/mimic-iii/nnmodel_state_dict.pt\"))\n",
        "\n",
        "for disease in train_data_loader.get_diseases():\n",
        "  p, r, f1 = run_sparse_baseline_model(train_data_loader, disease, test_annotation_path, test_cui_path)\n",
        "  sparse_baseline_p.append(p)\n",
        "  sparse_baseline_r.append(r)\n",
        "  sparse_baseline_f.append(f1)\n",
        "\n",
        "  p, r, f1 = run_svd_baseline_model(train_data_loader, disease, test_annotation_path, test_cui_path)\n",
        "  sparse_svd_p.append(p)\n",
        "  sparse_svd_r.append(r)\n",
        "  sparse_svd_f.append(f1)\n",
        "\n",
        "  p, r, f1 = run_dense_model(train_data_loader, disease, test_annotation_path, test_cui_path, model)\n",
        "  dense_p.append(p)\n",
        "  dense_r.append(r)\n",
        "  dense_f.append(f1)\n",
        "\n",
        "  p, r, f1 = run_dense_model_avg_states(train_data_loader, disease, test_annotation_path, test_cui_path, model)\n",
        "  dense_avg_p.append(p)\n",
        "  dense_avg_r.append(r)\n",
        "  dense_avg_f.append(f1)\n",
        "\n",
        "  p, r, f1 = run_w2v_embeddings_model(train_data_loader, disease, test_annotation_path, test_cui_path, W2Vmodel)\n",
        "  w2v_p.append(p)\n",
        "  w2v_r.append(r)\n",
        "  w2v_f.append(f1)\n",
        "\n",
        "  p, r, f1 = run_d2v_embeddings_model(train_data_loader, disease, test_annotation_path, test_cui_path, D2Vmodel)\n",
        "  d2v_p.append(p)\n",
        "  d2v_r.append(r)\n",
        "  d2v_f.append(f1)  "
      ],
      "metadata": {
        "id": "mM8IuRV3p_l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_baseline_p[:5], sparse_baseline_r[:5], sparse_baseline_f[:5]"
      ],
      "metadata": {
        "id": "5jy4LYujr33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_svd_p[:5], sparse_svd_r[:5], sparse_svd_f[:5]"
      ],
      "metadata": {
        "id": "80lX_Ybbt4T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_p[:5], dense_r[:5], dense_f[:5]"
      ],
      "metadata": {
        "id": "Up7vHNCpt-Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_avg_p[:5], dense_avg_r[:5], dense_avg_f[:5]"
      ],
      "metadata": {
        "id": "dPT7BblIOd25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_p[:5], w2v_r[:5], w2v_f[:5]"
      ],
      "metadata": {
        "id": "CZhW9dhMu4gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2v_p[:5], d2v_r[:5], d2v_f[:5]"
      ],
      "metadata": {
        "id": "ij9ZBp3zEodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(sparse_baseline_f), np.mean(sparse_svd_f), np.mean(dense_f), np.mean(dense_avg_f), np.mean(w2v_f), np.mean(d2v_f)"
      ],
      "metadata": {
        "id": "SQVfPhTjucwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(sparse_baseline_p), np.mean(sparse_svd_p), np.mean(dense_p), np.mean(dense_avg_p), np.mean(w2v_p), np.mean(d2v_p)"
      ],
      "metadata": {
        "id": "Hwm0HMYxvRmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(sparse_baseline_r), np.mean(sparse_svd_r), np.mean(dense_r), np.mean(dense_avg_r), np.mean(w2v_r), np.mean(d2v_r)"
      ],
      "metadata": {
        "id": "k9CHEvifvT4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving results\n",
        "\n",
        "# np.save(\"drive/MyDrive/mimic-iii/sparse_baseline_f.npy\", sparse_baseline_f)\n",
        "# np.save(\"drive/MyDrive/mimic-iii/sparse_svd_f.npy\", sparse_svd_f)\n",
        "# np.save(\"drive/MyDrive/mimic-iii/dense_f.npy\", dense_f)\n",
        "\n",
        "# np.save(\"drive/MyDrive/mimic-iii/sparse_baseline_p.npy\", sparse_baseline_p)\n",
        "# np.save(\"drive/MyDrive/mimic-iii/sparse_svd_p.npy\", sparse_svd_p)\n",
        "# np.save(\"drive/MyDrive/mimic-iii/dense_p.npy\", dense_p)\n",
        "\n",
        "# np.save(\"drive/MyDrive/mimic-iii/sparse_baseline_r.npy\", sparse_baseline_r)\n",
        "# np.save(\"drive/MyDrive/mimic-iii/sparse_svd_r.npy\", sparse_svd_r)\n",
        "# np.save(\"drive/MyDrive/mimic-iii/dense_r.npy\", dense_r)"
      ],
      "metadata": {
        "id": "Apm16aDWuP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x7_H2Sr4DbU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}