{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNModels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c7d3ARDXLrFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9fd29c-05e2-4f78-bd40-c8ccf2870714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "M0O3_WWD1pxN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Na-LsSzxqiff"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction configs:\n",
        "MIN_TOKEN_FREQ = 100\n",
        "MAX_TOKENS_IN_FILE = 10000\n",
        "MIN_EXAMPLES_PER_CODE = 1000\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "#yunxuan's\n",
        "CPT_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/CPTEVENTS.csv\"\n",
        "DIAGNOSIS_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "PROCEDURES_FILE_PATH = \"drive/MyDrive/MIMIC/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "CORPUS_FILE_PATH = \"drive/MyDrive/MIMIC/output_first_half_selected_cuis/\" #FILL IN PATH\n",
        "\n",
        "# CPT_FILE_PATH = \"drive/MyDrive/mimic-iii/CPTEVENTS.csv\"\n",
        "# DIAGNOSIS_FILE_PATH = \"drive/MyDrive/mimic-iii/DIAGNOSES_ICD.csv\"\n",
        "# PROCEDURES_FILE_PATH = \"drive/MyDrive/mimic-iii/PROCEDURES_ICD.csv\"\n",
        "# CORPUS_FILE_PATH = \"\" #FILL IN PATH"
      ],
      "metadata": {
        "id": "qYtRDkM41rcy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model configs\n",
        "criterion = nn.BCELoss()\n",
        "n_epochs = 75\n",
        "batch_size = 50"
      ],
      "metadata": {
        "id": "OVDwacnI9cb4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ICDLoader: \n",
        "  \"\"\"Load ICD billing codes labels for each patient\"\"\"\n",
        "\n",
        "  def __init__(self, corpus_file_path, cpt_file_path, diagnosis_file_path, procedures_file_path, min_examples_per_code, min_token_freq, max_tokens_in_file):\n",
        "    self.corpus_path = corpus_file_path\n",
        "    self.cpt_path = cpt_file_path\n",
        "    self.diagnosis_path = diagnosis_file_path\n",
        "    self.procedures_path = procedures_file_path\n",
        "   \n",
        "    self.max_tokens_in_file = max_tokens_in_file\n",
        "    self.min_examples_per_code = min_examples_per_code\n",
        "    self.min_token_freq = min_token_freq\n",
        "    self.patient2label_dict = None #mapping from patient -> ICD9 label codes\n",
        "    self.label2idx_dict = None #mapping from ICD9 label code -> embedding idx\n",
        "\n",
        "    self.token2int = {}\n",
        "\n",
        "  def df_make_code(self, df, icd_type, code_len, code_col):\n",
        "    #making special short string for codes\n",
        "    df['short_code'] = icd_type + \"_\" + df[code_col].astype(str).str[:code_len]\n",
        "    return df\n",
        "  \n",
        "  def get_label2freq_df(self, df, min_examples_per_code):\n",
        "    #get a filtered label codes -> frequency mapping table \n",
        "    label2freq_df = df[[\"SUBJECT_ID\", \"short_code\"]].groupby(\"short_code\").nunique()\n",
        "    label2freq_df = label2freq_df[label2freq_df[\"SUBJECT_ID\"]>min_examples_per_code]\n",
        "    label2freq_df.rename({\"SUBJECT_ID\": \"freq\"}, axis=1, inplace=True)\n",
        "    return label2freq_df.reset_index()\n",
        "\n",
        "  def get_patient2label_df(self, df, label2freq_df):\n",
        "    #get df of patient mapping to all their filtered ICD9 code labels (filtered by freq)\n",
        "    df_filtered = df[df[\"short_code\"].isin(label2freq_df[\"short_code\"])] \n",
        "    patient2label_df = df_filtered[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "                          .groupby(\"SUBJECT_ID\")\\\n",
        "                          .agg({'short_code':lambda sf: set(sf)})\n",
        "    patient2label_df.rename({\"short_code\":\"short_codes\"}, axis=1, inplace=True)          \n",
        "    return patient2label_df.reset_index()\n",
        "\n",
        "  def create_patient_label_vec(self, subj_id, patient2label_dict, label2idx_dict):\n",
        "    #make patient label vector\n",
        "    code_vec = [0]*len(label2idx_dict)\n",
        "    codes = patient2label_dict[subj_id]\n",
        "    for code in codes:\n",
        "      code_vec[label2idx_dict[code]] = 1\n",
        "    return code_vec\n",
        "\n",
        "  def make_cui_token2int_mapping(self):\n",
        "    #count tokens\n",
        "    token_count_dict = {}\n",
        "    for file in os.listdir(self.corpus_path):\n",
        "      text = open(os.path.join(self.corpus_path,file)).read()\n",
        "      tokens = [token for token in text.split()] #assume all cui in file splitted by space\n",
        "      if len(tokens) > self.max_tokens_in_file:\n",
        "        continue\n",
        "      else:\n",
        "        for token in tokens:\n",
        "          if token in token_count_dict:\n",
        "            token_count_dict[token] += 1\n",
        "          else:\n",
        "            token_count_dict[token] = 1\n",
        "    \n",
        "    #make token2int mapping\n",
        "    oov_idx = 0\n",
        "    idx = 1\n",
        "    self.token2int['oov_word'] = 0\n",
        "    for token, count in token_count_dict.items():\n",
        "      if count > self.min_token_freq:\n",
        "        self.token2int[token] = idx\n",
        "        idx += 1\n",
        "  \n",
        "  def create_cui_input_sequence(self, tokens):\n",
        "    #create cui_input_sequence from cui tokens\n",
        "    input = []\n",
        "    tokens_set = set(tokens)\n",
        "\n",
        "    for token in tokens_set:\n",
        "      if token in self.token2int:\n",
        "        input.append(self.token2int[token])\n",
        "      else:\n",
        "        input.append(self.token2int['oov_word'])\n",
        "\n",
        "    return input\n",
        "\n",
        "  def run(self):\n",
        "    #run everything\n",
        "\n",
        "    #codes init\n",
        "    cpt = pd.read_csv(self.cpt_path)\n",
        "    diagnosis = pd.read_csv(self.diagnosis_path)\n",
        "    procedures = pd.read_csv(self.procedures_path)\n",
        "\n",
        "    #codes init\n",
        "    cpt = self.df_make_code(cpt, 'cpt', 5, 'CPT_NUMBER')\n",
        "    diagnosis = self.df_make_code(diagnosis, 'diag', 3, 'ICD9_CODE')\n",
        "    procedures = self.df_make_code(procedures, 'proc', 2, 'ICD9_CODE')\n",
        "\n",
        "    #codes init\n",
        "    all_codes = cpt[[\"SUBJECT_ID\", \"short_code\"]]\\\n",
        "              .append(diagnosis[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\\\n",
        "              .append(procedures[[\"SUBJECT_ID\", \"short_code\"]], ignore_index=True)\n",
        "\n",
        "    #codes init\n",
        "    label2freq_df = self.get_label2freq_df(all_codes, self.min_examples_per_code)\n",
        "    patient2label_df = self.get_patient2label_df(all_codes, label2freq_df)\n",
        "    \n",
        "    #codes init\n",
        "    self.label2idx_dict = dict(label2freq_df.reset_index()[[\"short_code\", \"index\"]].values)\n",
        "    self.patient2label_dict = dict(patient2label_df.values)\n",
        "\n",
        "    #cui init\n",
        "    self.make_cui_token2int_mapping()\n",
        "\n",
        "    codes = []\n",
        "    cui_inputs = []\n",
        "\n",
        "    #processing\n",
        "    for file in os.listdir(self.corpus_path): #list files to run\n",
        "      text = open(os.path.join(self.corpus_path, file)).read()\n",
        "      tokens = [token for token in text.split()]\n",
        "      if len(tokens) > self.max_tokens_in_file: #cui filter\n",
        "        continue\n",
        "\n",
        "      subj_id = int(re.findall('\\d+(?=.txt)', file)[0])\n",
        "      if subj_id not in self.patient2label_dict: #icd9 filter\n",
        "        continue\n",
        "\n",
        "      #icd9 process  \n",
        "      code_vec = self.create_patient_label_vec(subj_id, self.patient2label_dict, \n",
        "                                               self.label2idx_dict)\n",
        "      if sum(code_vec) == 0:\n",
        "        continue\n",
        "      codes.append(code_vec)\n",
        "\n",
        "      #cui process\n",
        "      cui_input = self.create_cui_input_sequence(tokens)\n",
        "      cui_inputs.append(cui_input)\n",
        "\n",
        "    return cui_inputs, codes"
      ],
      "metadata": {
        "id": "vNZSYMog1y7v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = ICDLoader(CORPUS_FILE_PATH, CPT_FILE_PATH, DIAGNOSIS_FILE_PATH, PROCEDURES_FILE_PATH, MIN_EXAMPLES_PER_CODE, MIN_TOKEN_FREQ, MAX_TOKENS_IN_FILE)\n",
        "cui_inputs, codes = loader.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QChkvpCd10_p",
        "outputId": "1dbddf66-94f9-42c8-fe7e-120addeaa2e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (4,5,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = max([len(patient) for patient in cui_inputs])\n",
        "emb_dim = len(loader.token2int)\n",
        "n_class = len(loader.label2idx_dict)"
      ],
      "metadata": {
        "id": "5asmd-8daX_q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-JcVtdG6cevL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cui_inputs,codes, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sFdSksIsWSrV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x # shape n_sample x padded length\n",
        "        self.y = y # shape n_sample x n classes [[0,1,0,0], [1,1,1,0]]\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        return(self.x[index], self.y[index])"
      ],
      "metadata": {
        "id": "ZN4e24eH7ESQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = customDataset(X_train, y_train)\n",
        "test_dataset = customDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "eYi0vuIRXt4p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    sequences, labels = zip(*data)\n",
        "    y = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    n = len(sequences)\n",
        "    x = torch.zeros((n, maxlen), dtype=torch.long)\n",
        "    \n",
        "    for patient, cuis in enumerate(sequences):\n",
        "      len_cuis = len(cuis)\n",
        "      x[patient][:len_cuis] = torch.tensor(cuis)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "jEE-XLdKWm0s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "pLvc4ost7lCD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_representation(nn.Module):\n",
        "  def __init__(self, in_dim, n_diseases):\n",
        "    super(NN_representation, self).__init__()  \n",
        "    self.emb = nn.Embedding(num_embeddings= in_dim, embedding_dim= 300)\n",
        "    self.avg = nn.AdaptiveMaxPool1d(1)\n",
        "    self.hidden = nn.Linear(300, 1000)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.final = nn.Linear(1000, n_diseases)\n",
        "    self.act2 = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    temp = self.emb(x)\n",
        "    #print(f\"after emb, {temp.shape}\")\n",
        "    temp = torch.permute(temp, (0,2,1))\n",
        "    #print(f\"after permute, {temp.shape}\")\n",
        "    temp = self.avg(temp)\n",
        "    #print(f\"after avg, {temp.shape}\")\n",
        "    temp = temp.squeeze(-1)\n",
        "    #print(f\"after squeeze, {temp.shape}\")\n",
        "    temp = self.hidden(temp)\n",
        "    #print(f\"after hidden, {temp.shape}\")\n",
        "    temp = self.act1(temp)\n",
        "    #print(f\"after relu, {temp.shape}\")\n",
        "    temp = self.final(temp)\n",
        "    #print(f\"after linear hidden, {temp.shape}\")\n",
        "    res = self.act2(temp)\n",
        "    return res"
      ],
      "metadata": {
        "id": "RbIUQDSd0TGw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nnmodel =NN_representation(emb_dim, n_class)\n",
        "optimizer = torch.optim.RMSprop(nnmodel.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tO36rB2kaAyS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, n_epochs):\n",
        "  model.train()\n",
        "  for epoch in range(n_epochs):\n",
        "    current_loss = 0\n",
        "    for current_x, current_y in loader:\n",
        "      pred = model(current_x)\n",
        "      loss = criterion(pred, current_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    train_loss = current_loss/len(loader)\n",
        "    print(f\"after epoch {epoch}, the training loss is {train_loss}\")\n"
      ],
      "metadata": {
        "id": "Tf7_D2aaw2rn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(nnmodel, train_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAqZDtAcgdAN",
        "outputId": "32a8ef8c-9359-494e-9801-ca3691499b88"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after epoch 0, the training loss is 0.37709851495244284\n",
            "after epoch 1, the training loss is 0.22488768060098996\n",
            "after epoch 2, the training loss is 0.22125912525437094\n",
            "after epoch 3, the training loss is 0.22041435878385196\n",
            "after epoch 4, the training loss is 0.21335617791522632\n",
            "after epoch 5, the training loss is 0.20902928439053622\n",
            "after epoch 6, the training loss is 0.20389360121705316\n",
            "after epoch 7, the training loss is 0.19572935659777035\n",
            "after epoch 8, the training loss is 0.19551328772848303\n",
            "after epoch 9, the training loss is 0.19071940671313892\n",
            "after epoch 10, the training loss is 0.18724086609753696\n",
            "after epoch 11, the training loss is 0.18752758543599735\n",
            "after epoch 12, the training loss is 0.1855496337467974\n",
            "after epoch 13, the training loss is 0.18654481998898767\n",
            "after epoch 14, the training loss is 0.18475250222466208\n",
            "after epoch 15, the training loss is 0.1786462739109993\n",
            "after epoch 16, the training loss is 0.18320807746865533\n",
            "after epoch 17, the training loss is 0.17855520004575903\n",
            "after epoch 18, the training loss is 0.18017903579906983\n",
            "after epoch 19, the training loss is 0.17618124796585602\n",
            "after epoch 20, the training loss is 0.17795207554643805\n",
            "after epoch 21, the training loss is 0.1759885691783645\n",
            "after epoch 22, the training loss is 0.1724031310189854\n",
            "after epoch 23, the training loss is 0.17348340356891806\n",
            "after epoch 24, the training loss is 0.173379316248677\n",
            "after epoch 25, the training loss is 0.17356239394708114\n",
            "after epoch 26, the training loss is 0.17085444656285373\n",
            "after epoch 27, the training loss is 0.16986088116060605\n",
            "after epoch 28, the training loss is 0.16877138411456888\n",
            "after epoch 29, the training loss is 0.169549436731772\n",
            "after epoch 30, the training loss is 0.16869853911074725\n",
            "after epoch 31, the training loss is 0.1679367714307525\n",
            "after epoch 32, the training loss is 0.16881624947894702\n",
            "after epoch 33, the training loss is 0.16582120616327634\n",
            "after epoch 34, the training loss is 0.16632216491482474\n",
            "after epoch 35, the training loss is 0.16585527766834607\n",
            "after epoch 36, the training loss is 0.16342320022257892\n",
            "after epoch 37, the training loss is 0.1629104461859573\n",
            "after epoch 38, the training loss is 0.1653000143441287\n",
            "after epoch 39, the training loss is 0.16327929564497687\n",
            "after epoch 40, the training loss is 0.1623497327620333\n",
            "after epoch 41, the training loss is 0.16122222082181412\n",
            "after epoch 42, the training loss is 0.16031112521886826\n",
            "after epoch 43, the training loss is 0.16190983084115115\n",
            "after epoch 44, the training loss is 0.15926231782544742\n",
            "after epoch 45, the training loss is 0.16175815056670795\n",
            "after epoch 46, the training loss is 0.15767125378955493\n",
            "after epoch 47, the training loss is 0.15826131267981094\n",
            "after epoch 48, the training loss is 0.15745218436826358\n",
            "after epoch 49, the training loss is 0.15882091495123776\n",
            "after epoch 50, the training loss is 0.15853088416836478\n",
            "after epoch 51, the training loss is 0.1586007997393608\n",
            "after epoch 52, the training loss is 0.15663040869615294\n",
            "after epoch 53, the training loss is 0.15623203766616908\n",
            "after epoch 54, the training loss is 0.15307948196476157\n",
            "after epoch 55, the training loss is 0.1543083367022601\n",
            "after epoch 56, the training loss is 0.15529578788713974\n",
            "after epoch 57, the training loss is 0.1526329256594181\n",
            "after epoch 58, the training loss is 0.15574227476661856\n",
            "after epoch 59, the training loss is 0.15349737216125836\n",
            "after epoch 60, the training loss is 0.15206022628329016\n",
            "after epoch 61, the training loss is 0.15188939666206186\n",
            "after epoch 62, the training loss is 0.15247968144037508\n",
            "after epoch 63, the training loss is 0.15254369378089905\n",
            "after epoch 64, the training loss is 0.15259371562437576\n",
            "after epoch 65, the training loss is 0.15000459348613565\n",
            "after epoch 66, the training loss is 0.14945540848103436\n",
            "after epoch 67, the training loss is 0.15102252093228427\n",
            "after epoch 68, the training loss is 0.14885010502555154\n",
            "after epoch 69, the training loss is 0.14884876053441654\n",
            "after epoch 70, the training loss is 0.1500715748830275\n",
            "after epoch 71, the training loss is 0.1474762958559123\n",
            "after epoch 72, the training loss is 0.14910447936166416\n",
            "after epoch 73, the training loss is 0.1486243216151541\n",
            "after epoch 74, the training loss is 0.14734933626922694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, loader):\n",
        "  model.eval()\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for current_x, current_y in loader:\n",
        "      preds = model(current_x).detach().numpy()\n",
        "      preds_labels = preds>0.5\n",
        "      y_pred.append(preds_labels)\n",
        "      y_true.append(current_y.numpy())\n",
        "  y_pred = np.vstack(y_pred)  \n",
        "  y_true = np.vstack(y_true)        \n",
        "\n",
        "  p,r,f,_ = precision_recall_fscore_support(y_pred, y_true, average='micro')\n",
        "  acc = accuracy_score(y_pred, y_true)\n",
        "  return p,r,f,acc"
      ],
      "metadata": {
        "id": "mEsfpNvy26K3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy = test(nnmodel, test_loader)"
      ],
      "metadata": {
        "id": "LVkdbKlqkbpW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irc9lEZUlG5H",
        "outputId": "f15eff0b-ca80-475a-9cab-8abc8d8f72e5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2882504841833441,\n",
              " 0.7014925373134329,\n",
              " 0.40860215053763443,\n",
              " 0.06343283582089553)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_train, recall_train, f1_train, accuracy_train = test(nnmodel, train_loader)"
      ],
      "metadata": {
        "id": "bwnoyfB18DRf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_train, recall_train, f1_train, accuracy_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MazX9aY5luaN",
        "outputId": "69675e5a-4a65-49ce-939c-0da3a548dd1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2987093764577826,\n",
              " 0.7377112135176651,\n",
              " 0.42523519645821806,\n",
              " 0.05337078651685393)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wW74DMnw8Kmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}